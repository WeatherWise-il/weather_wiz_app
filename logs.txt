* 
* ==> Audit <==
* |--------------|----------------------------------|----------|-----------------|---------|---------------------|---------------------|
|   Command    |               Args               | Profile  |      User       | Version |     Start Time      |      End Time       |
|--------------|----------------------------------|----------|-----------------|---------|---------------------|---------------------|
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 16 Feb 24 09:14 IST | 16 Feb 24 09:14 IST |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 16 Feb 24 10:57 IST | 16 Feb 24 10:57 IST |
| dashboard    |                                  | minikube | michaelshechter | v1.32.0 | 16 Feb 24 11:41 IST |                     |
| addons       | enable ingress                   | minikube | michaelshechter | v1.32.0 | 19 Feb 24 21:10 IST |                     |
| addons       | enable ingress                   | minikube | michaelshechter | v1.32.0 | 19 Feb 24 21:17 IST |                     |
| start        |                                  | minikube | michaelshechter | v1.32.0 | 19 Feb 24 21:24 IST | 19 Feb 24 21:24 IST |
| addons       | enable ingress                   | minikube | michaelshechter | v1.32.0 | 19 Feb 24 21:25 IST | 19 Feb 24 21:27 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 20 Feb 24 20:58 IST | 20 Feb 24 20:58 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 21 Feb 24 14:43 IST | 21 Feb 24 14:43 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 23 Feb 24 16:03 IST | 23 Feb 24 16:03 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 24 Feb 24 16:38 IST | 24 Feb 24 16:38 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 24 Feb 24 16:59 IST |                     |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 24 Feb 24 16:59 IST | 24 Feb 24 16:59 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 24 Feb 24 16:59 IST | 24 Feb 24 16:59 IST |
| start        |                                  | minikube | michaelshechter | v1.32.0 | 24 Feb 24 17:00 IST | 24 Feb 24 17:01 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 28 Feb 24 19:21 IST | 28 Feb 24 19:21 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 01 Mar 24 12:29 IST | 01 Mar 24 12:29 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 01 Mar 24 12:32 IST | 01 Mar 24 12:32 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 01 Mar 24 22:43 IST | 01 Mar 24 22:43 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 02 Mar 24 19:19 IST | 02 Mar 24 19:19 IST |
| image        | load mysql_db_img:1.0            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:21 IST |                     |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:22 IST | 03 Mar 24 15:22 IST |
| image        | load mysql_db_img:1.0            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:24 IST |                     |
| image        | load mysql_db_img:1.0            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:27 IST |                     |
| image        | load mysql_db_img:1.0            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:29 IST |                     |
| image        | load 0ac289292fd7                | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:31 IST | 03 Mar 24 15:31 IST |
| image        | load mysql_db_img:1.0            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:31 IST |                     |
| image        | load app_flask:1.1               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:37 IST |                     |
| stop         |                                  | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:37 IST | 03 Mar 24 15:37 IST |
| start        |                                  | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:38 IST | 03 Mar 24 15:38 IST |
| image        | load mysql_db_img:1.0            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 15:38 IST |                     |
| image        | load mysql_db_img:1.1            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:01 IST |                     |
| image        | load mysql_db_img:1.1            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:42 IST |                     |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:47 IST | 03 Mar 24 16:47 IST |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:48 IST | 03 Mar 24 16:48 IST |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:51 IST | 03 Mar 24 16:51 IST |
| start        |                                  | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:52 IST | 03 Mar 24 16:52 IST |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:52 IST | 03 Mar 24 16:52 IST |
| image        | load mysql_db_img:1.1            | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:52 IST |                     |
| image        | build -t mysql_img:1.2 .         | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:54 IST | 03 Mar 24 16:54 IST |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:55 IST | 03 Mar 24 16:55 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 03 Mar 24 16:56 IST | 03 Mar 24 16:56 IST |
| image        | build -t mysql_img:1.2 .         | minikube | michaelshechter | v1.32.0 | 03 Mar 24 17:16 IST |                     |
| image        | build -t flask_img:1.0 .         | minikube | michaelshechter | v1.32.0 | 03 Mar 24 17:16 IST | 03 Mar 24 17:16 IST |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 03 Mar 24 17:17 IST | 03 Mar 24 17:17 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 03 Mar 24 19:56 IST | 03 Mar 24 19:56 IST |
| ip           |                                  | minikube | michaelshechter | v1.32.0 | 03 Mar 24 21:00 IST | 03 Mar 24 21:00 IST |
| service      | flask-service                    | minikube | michaelshechter | v1.32.0 | 03 Mar 24 21:03 IST |                     |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 19 Mar 24 20:53 IST | 19 Mar 24 20:53 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 28 Mar 24 13:27 IST | 28 Mar 24 13:27 IST |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 06 Apr 24 17:05 IDT | 06 Apr 24 17:05 IDT |
| update-check |                                  | minikube | michaelshechter | v1.32.0 | 09 Apr 24 18:11 IDT | 09 Apr 24 18:11 IDT |
| start        |                                  | minikube | michaelshechter | v1.32.0 | 24 May 24 15:47 IDT | 24 May 24 15:47 IDT |
| image        | load                             | minikube | michaelshechter | v1.32.0 | 24 May 24 16:02 IDT |                     |
|              | weather_wiz_app-flask_app:latest |          |                 |         |                     |                     |
| image        | load backend_app:1.0             | minikube | michaelshechter | v1.32.0 | 24 May 24 16:06 IDT |                     |
| cache        | add backend_app:1.0              | minikube | michaelshechter | v1.32.0 | 24 May 24 16:09 IDT |                     |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 24 May 24 16:10 IDT | 24 May 24 16:10 IDT |
| image        | ls                               | minikube | michaelshechter | v1.32.0 | 24 May 24 16:24 IDT | 24 May 24 16:24 IDT |
| image        | load backend_app:1.0             | minikube | michaelshechter | v1.32.0 | 24 May 24 16:28 IDT |                     |
| cache        | add backend_app:1.0              | minikube | michaelshechter | v1.32.0 | 24 May 24 16:31 IDT |                     |
|--------------|----------------------------------|----------|-----------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/05/24 15:47:20
Running on machine: Michaels-MacBook-Pro
Binary: Built with gc go1.21.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0524 15:47:20.127250   40902 out.go:296] Setting OutFile to fd 1 ...
I0524 15:47:20.127436   40902 out.go:348] isatty.IsTerminal(1) = true
I0524 15:47:20.127438   40902 out.go:309] Setting ErrFile to fd 2...
I0524 15:47:20.127441   40902 out.go:348] isatty.IsTerminal(2) = true
I0524 15:47:20.127613   40902 root.go:338] Updating PATH: /Users/michaelshechter/.minikube/bin
W0524 15:47:20.127678   40902 root.go:314] Error reading config file at /Users/michaelshechter/.minikube/config/config.json: open /Users/michaelshechter/.minikube/config/config.json: no such file or directory
I0524 15:47:20.130186   40902 out.go:303] Setting JSON to false
I0524 15:47:20.161517   40902 start.go:128] hostinfo: {"hostname":"Michaels-MacBook-Pro.local","uptime":4501845,"bootTime":1712052995,"procs":698,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.4.1","kernelVersion":"23.4.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"17326d40-5830-5d32-a782-c1a6de59e5f8"}
W0524 15:47:20.161630   40902 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0524 15:47:20.175087   40902 out.go:177] 😄  minikube v1.32.0 on Darwin 14.4.1 (arm64)
I0524 15:47:20.187053   40902 notify.go:220] Checking for updates...
I0524 15:47:20.187406   40902 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0524 15:47:20.188324   40902 driver.go:378] Setting default libvirt URI to qemu:///system
I0524 15:47:20.252444   40902 docker.go:122] docker version: linux-26.0.0:Docker Desktop 4.29.0 (145265)
I0524 15:47:20.252734   40902 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0524 15:47:20.491116   40902 lock.go:35] WriteFile acquiring /Users/michaelshechter/.minikube/last_update_check: {Name:mk1d6d78906db177c41bf09f1c0821f50f55783c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 15:47:20.498013   40902 out.go:177] 🎉  minikube 1.33.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.33.1
I0524 15:47:20.503100   40902 out.go:177] 💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0524 15:47:20.561440   40902 info.go:266] docker info: {ID:108283d4-f42c-49d0-a076-f5e7402abded Containers:64 ContainersRunning:22 ContainersPaused:0 ContainersStopped:42 Images:201 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:true NFd:213 OomKillDisable:false NGoroutines:328 SystemTime:2024-05-24 12:47:20.537537594 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.22-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8326459392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/michaelshechter/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:tkl5q0snv7xjlwoa49mzlhoj6 NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:tkl5q0snv7xjlwoa49mzlhoj6]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/michaelshechter/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:/Users/michaelshechter/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:/Users/michaelshechter/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-debug] ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:/Users/michaelshechter/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-dev] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/michaelshechter/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/michaelshechter/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-feedback] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/michaelshechter/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/michaelshechter/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Err:failed to fetch metadata: fork/exec /Users/michaelshechter/.docker/cli-plugins/docker-scan: no such file or directory Name:scan Path:/Users/michaelshechter/.docker/cli-plugins/docker-scan] map[Name:scout Path:/Users/michaelshechter/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0524 15:47:20.570734   40902 out.go:177] ✨  Using the docker driver based on existing profile
I0524 15:47:20.574926   40902 start.go:298] selected driver: docker
I0524 15:47:20.574931   40902 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:7902 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0524 15:47:20.574991   40902 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0524 15:47:20.575114   40902 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0524 15:47:20.674194   40902 info.go:266] docker info: {ID:108283d4-f42c-49d0-a076-f5e7402abded Containers:64 ContainersRunning:22 ContainersPaused:0 ContainersStopped:42 Images:201 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:true NFd:213 OomKillDisable:false NGoroutines:328 SystemTime:2024-05-24 12:47:20.65460172 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.22-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8326459392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/michaelshechter/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:tkl5q0snv7xjlwoa49mzlhoj6 NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:tkl5q0snv7xjlwoa49mzlhoj6]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/michaelshechter/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:/Users/michaelshechter/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:/Users/michaelshechter/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-debug] ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:/Users/michaelshechter/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-dev] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/michaelshechter/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/michaelshechter/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-feedback] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/michaelshechter/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/michaelshechter/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Err:failed to fetch metadata: fork/exec /Users/michaelshechter/.docker/cli-plugins/docker-scan: no such file or directory Name:scan Path:/Users/michaelshechter/.docker/cli-plugins/docker-scan] map[Name:scout Path:/Users/michaelshechter/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0524 15:47:20.675027   40902 cni.go:84] Creating CNI manager for ""
I0524 15:47:20.675185   40902 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0524 15:47:20.675191   40902 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:7902 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0524 15:47:20.680491   40902 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0524 15:47:20.688734   40902 cache.go:121] Beginning downloading kic base image for docker with docker
I0524 15:47:20.698407   40902 out.go:177] 🚜  Pulling base image ...
I0524 15:47:20.702812   40902 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0524 15:47:20.702942   40902 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0524 15:47:20.702997   40902 preload.go:148] Found local preload: /Users/michaelshechter/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4
I0524 15:47:20.703004   40902 cache.go:56] Caching tarball of preloaded images
I0524 15:47:20.703885   40902 preload.go:174] Found /Users/michaelshechter/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0524 15:47:20.703914   40902 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0524 15:47:20.704250   40902 profile.go:148] Saving config to /Users/michaelshechter/.minikube/profiles/minikube/config.json ...
I0524 15:47:20.791205   40902 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0524 15:47:20.791255   40902 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0524 15:47:20.792253   40902 cache.go:194] Successfully downloaded all kic artifacts
I0524 15:47:20.792937   40902 start.go:365] acquiring machines lock for minikube: {Name:mk26bab3af03453ef6f1bd687c96bc40cfd5ea74 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0524 15:47:20.793184   40902 start.go:369] acquired machines lock for "minikube" in 233.292µs
I0524 15:47:20.793480   40902 start.go:96] Skipping create...Using existing machine configuration
I0524 15:47:20.793717   40902 fix.go:54] fixHost starting: 
I0524 15:47:20.793956   40902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 15:47:20.839852   40902 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0524 15:47:20.839887   40902 fix.go:128] unexpected machine state, will restart: <nil>
I0524 15:47:20.846372   40902 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0524 15:47:20.855648   40902 cli_runner.go:164] Run: docker start minikube
I0524 15:47:21.043611   40902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 15:47:21.093087   40902 kic.go:430] container "minikube" state is running.
I0524 15:47:21.094460   40902 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0524 15:47:21.144627   40902 profile.go:148] Saving config to /Users/michaelshechter/.minikube/profiles/minikube/config.json ...
I0524 15:47:21.145021   40902 machine.go:88] provisioning docker machine ...
I0524 15:47:21.145201   40902 ubuntu.go:169] provisioning hostname "minikube"
I0524 15:47:21.145519   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:21.195379   40902 main.go:141] libmachine: Using SSH client type: native
I0524 15:47:21.196168   40902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1031caf80] 0x1031cd6f0 <nil>  [] 0s} 127.0.0.1 58456 <nil> <nil>}
I0524 15:47:21.196176   40902 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0524 15:47:21.197573   40902 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0524 15:47:24.355509   40902 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0524 15:47:24.355784   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:24.407868   40902 main.go:141] libmachine: Using SSH client type: native
I0524 15:47:24.408201   40902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1031caf80] 0x1031cd6f0 <nil>  [] 0s} 127.0.0.1 58456 <nil> <nil>}
I0524 15:47:24.408209   40902 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0524 15:47:24.519829   40902 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0524 15:47:24.519843   40902 ubuntu.go:175] set auth options {CertDir:/Users/michaelshechter/.minikube CaCertPath:/Users/michaelshechter/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/michaelshechter/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/michaelshechter/.minikube/machines/server.pem ServerKeyPath:/Users/michaelshechter/.minikube/machines/server-key.pem ClientKeyPath:/Users/michaelshechter/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/michaelshechter/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/michaelshechter/.minikube}
I0524 15:47:24.519884   40902 ubuntu.go:177] setting up certificates
I0524 15:47:24.519902   40902 provision.go:83] configureAuth start
I0524 15:47:24.519994   40902 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0524 15:47:24.569119   40902 provision.go:138] copyHostCerts
I0524 15:47:24.569891   40902 exec_runner.go:144] found /Users/michaelshechter/.minikube/ca.pem, removing ...
I0524 15:47:24.570112   40902 exec_runner.go:203] rm: /Users/michaelshechter/.minikube/ca.pem
I0524 15:47:24.570227   40902 exec_runner.go:151] cp: /Users/michaelshechter/.minikube/certs/ca.pem --> /Users/michaelshechter/.minikube/ca.pem (1103 bytes)
I0524 15:47:24.570568   40902 exec_runner.go:144] found /Users/michaelshechter/.minikube/cert.pem, removing ...
I0524 15:47:24.570571   40902 exec_runner.go:203] rm: /Users/michaelshechter/.minikube/cert.pem
I0524 15:47:24.570640   40902 exec_runner.go:151] cp: /Users/michaelshechter/.minikube/certs/cert.pem --> /Users/michaelshechter/.minikube/cert.pem (1147 bytes)
I0524 15:47:24.570978   40902 exec_runner.go:144] found /Users/michaelshechter/.minikube/key.pem, removing ...
I0524 15:47:24.570980   40902 exec_runner.go:203] rm: /Users/michaelshechter/.minikube/key.pem
I0524 15:47:24.571035   40902 exec_runner.go:151] cp: /Users/michaelshechter/.minikube/certs/key.pem --> /Users/michaelshechter/.minikube/key.pem (1679 bytes)
I0524 15:47:24.571264   40902 provision.go:112] generating server cert: /Users/michaelshechter/.minikube/machines/server.pem ca-key=/Users/michaelshechter/.minikube/certs/ca.pem private-key=/Users/michaelshechter/.minikube/certs/ca-key.pem org=michaelshechter.minikube san=[192.168.58.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0524 15:47:24.785314   40902 provision.go:172] copyRemoteCerts
I0524 15:47:24.785810   40902 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0524 15:47:24.785859   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:24.836689   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:24.920505   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/machines/server.pem --> /etc/docker/server.pem (1224 bytes)
I0524 15:47:24.941104   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0524 15:47:24.957125   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1103 bytes)
I0524 15:47:24.972728   40902 provision.go:86] duration metric: configureAuth took 452.821917ms
I0524 15:47:24.972739   40902 ubuntu.go:193] setting minikube options for container-runtime
I0524 15:47:24.972896   40902 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0524 15:47:24.972967   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:25.021098   40902 main.go:141] libmachine: Using SSH client type: native
I0524 15:47:25.021437   40902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1031caf80] 0x1031cd6f0 <nil>  [] 0s} 127.0.0.1 58456 <nil> <nil>}
I0524 15:47:25.021442   40902 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0524 15:47:25.130024   40902 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0524 15:47:25.147209   40902 ubuntu.go:71] root file system type: overlay
I0524 15:47:25.147314   40902 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0524 15:47:25.147417   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:25.196385   40902 main.go:141] libmachine: Using SSH client type: native
I0524 15:47:25.196704   40902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1031caf80] 0x1031cd6f0 <nil>  [] 0s} 127.0.0.1 58456 <nil> <nil>}
I0524 15:47:25.196750   40902 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0524 15:47:25.325183   40902 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0524 15:47:25.325280   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:25.383401   40902 main.go:141] libmachine: Using SSH client type: native
I0524 15:47:25.383719   40902 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1031caf80] 0x1031cd6f0 <nil>  [] 0s} 127.0.0.1 58456 <nil> <nil>}
I0524 15:47:25.383732   40902 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0524 15:47:25.505969   40902 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0524 15:47:25.505983   40902 machine.go:91] provisioned docker machine in 4.360980917s
I0524 15:47:25.505988   40902 start.go:300] post-start starting for "minikube" (driver="docker")
I0524 15:47:25.505994   40902 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0524 15:47:25.506100   40902 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0524 15:47:25.506149   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:25.557624   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:25.637654   40902 ssh_runner.go:195] Run: cat /etc/os-release
I0524 15:47:25.640556   40902 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0524 15:47:25.640574   40902 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0524 15:47:25.640583   40902 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0524 15:47:25.640587   40902 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0524 15:47:25.640592   40902 filesync.go:126] Scanning /Users/michaelshechter/.minikube/addons for local assets ...
I0524 15:47:25.640699   40902 filesync.go:126] Scanning /Users/michaelshechter/.minikube/files for local assets ...
I0524 15:47:25.640741   40902 start.go:303] post-start completed in 134.750334ms
I0524 15:47:25.640807   40902 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0524 15:47:25.640850   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:25.694736   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:25.786561   40902 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0524 15:47:25.791806   40902 fix.go:56] fixHost completed within 4.998351292s
I0524 15:47:25.791817   40902 start.go:83] releasing machines lock for "minikube", held for 4.998657209s
I0524 15:47:25.791889   40902 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0524 15:47:25.845052   40902 ssh_runner.go:195] Run: cat /version.json
I0524 15:47:25.845118   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:25.846438   40902 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0524 15:47:25.846906   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:25.903261   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:25.903257   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:26.259174   40902 ssh_runner.go:195] Run: systemctl --version
I0524 15:47:26.267953   40902 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0524 15:47:26.273470   40902 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0524 15:47:26.290060   40902 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0524 15:47:26.290277   40902 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0524 15:47:26.296044   40902 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0524 15:47:26.296062   40902 start.go:472] detecting cgroup driver to use...
I0524 15:47:26.296085   40902 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0524 15:47:26.296924   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0524 15:47:26.306470   40902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0524 15:47:26.312576   40902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0524 15:47:26.318287   40902 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0524 15:47:26.318366   40902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0524 15:47:26.325369   40902 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0524 15:47:26.330960   40902 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0524 15:47:26.336402   40902 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0524 15:47:26.341762   40902 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0524 15:47:26.347827   40902 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0524 15:47:26.353473   40902 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0524 15:47:26.361799   40902 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0524 15:47:26.367550   40902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 15:47:26.411027   40902 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0524 15:47:26.464639   40902 start.go:472] detecting cgroup driver to use...
I0524 15:47:26.464680   40902 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0524 15:47:26.464938   40902 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0524 15:47:26.472968   40902 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0524 15:47:26.473096   40902 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0524 15:47:26.481798   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0524 15:47:26.492962   40902 ssh_runner.go:195] Run: which cri-dockerd
I0524 15:47:26.495503   40902 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0524 15:47:26.501700   40902 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0524 15:47:26.512673   40902 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0524 15:47:26.577205   40902 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0524 15:47:26.674280   40902 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0524 15:47:26.674441   40902 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0524 15:47:26.686406   40902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 15:47:26.757376   40902 ssh_runner.go:195] Run: sudo systemctl restart docker
I0524 15:47:27.056874   40902 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0524 15:47:27.099442   40902 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0524 15:47:27.140482   40902 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0524 15:47:27.180227   40902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 15:47:27.220145   40902 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0524 15:47:27.237612   40902 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 15:47:27.274728   40902 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0524 15:47:27.420439   40902 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0524 15:47:27.421403   40902 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0524 15:47:27.424046   40902 start.go:540] Will wait 60s for crictl version
I0524 15:47:27.424137   40902 ssh_runner.go:195] Run: which crictl
I0524 15:47:27.426399   40902 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0524 15:47:27.516671   40902 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0524 15:47:27.516803   40902 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0524 15:47:27.577736   40902 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0524 15:47:27.608180   40902 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0524 15:47:27.608738   40902 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0524 15:47:27.725839   40902 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0524 15:47:27.726371   40902 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0524 15:47:27.729361   40902 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0524 15:47:27.736154   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0524 15:47:27.783972   40902 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0524 15:47:27.784039   40902 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0524 15:47:27.801897   40902 docker.go:671] Got preloaded images: -- stdout --
flask_img:1.0
mysql_img:1.2
app_flask:1.1
nginx:latest
quay.io/argoproj/argocd:v2.10.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.71.2
quay.io/prometheus-operator/prometheus-operator:v0.71.2
grafana/grafana:10.3.1
arm64v8/mysql:8
quay.io/prometheus/prometheus:v2.49.1
redis:7.0.14-alpine
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
gcr.io/google-samples/hello-app:2.0
gcr.io/google-samples/hello-app:1.0
<none>:<none>
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
quay.io/kiwigrid/k8s-sidecar:1.25.2
quay.io/prometheus/alertmanager:v0.26.0
ghcr.io/dexidp/dex:v2.37.0
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
nanajanashia/argocd-app:1.2
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2
ghost:2.6-alpine
wordpress:4.8-apache

-- /stdout --
I0524 15:47:27.802313   40902 docker.go:601] Images already preloaded, skipping extraction
I0524 15:47:27.802579   40902 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0524 15:47:27.816719   40902 docker.go:671] Got preloaded images: -- stdout --
flask_img:1.0
mysql_img:1.2
app_flask:1.1
nginx:latest
quay.io/argoproj/argocd:v2.10.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.71.2
quay.io/prometheus-operator/prometheus-operator:v0.71.2
grafana/grafana:10.3.1
arm64v8/mysql:8
quay.io/prometheus/prometheus:v2.49.1
redis:7.0.14-alpine
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
gcr.io/google-samples/hello-app:2.0
gcr.io/google-samples/hello-app:1.0
<none>:<none>
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
quay.io/kiwigrid/k8s-sidecar:1.25.2
quay.io/prometheus/alertmanager:v0.26.0
ghcr.io/dexidp/dex:v2.37.0
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
nanajanashia/argocd-app:1.2
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2
ghost:2.6-alpine
wordpress:4.8-apache

-- /stdout --
I0524 15:47:27.816736   40902 cache_images.go:84] Images are preloaded, skipping loading
I0524 15:47:27.817065   40902 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0524 15:47:27.926571   40902 cni.go:84] Creating CNI manager for ""
I0524 15:47:27.926580   40902 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0524 15:47:27.926793   40902 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0524 15:47:27.926808   40902 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0524 15:47:27.926907   40902 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0524 15:47:27.926948   40902 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0524 15:47:27.927067   40902 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0524 15:47:27.934031   40902 binaries.go:44] Found k8s binaries, skipping transfer
I0524 15:47:27.934125   40902 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0524 15:47:27.939377   40902 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0524 15:47:27.950127   40902 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0524 15:47:27.959908   40902 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0524 15:47:27.970014   40902 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0524 15:47:27.972751   40902 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0524 15:47:27.979825   40902 certs.go:56] Setting up /Users/michaelshechter/.minikube/profiles/minikube for IP: 192.168.58.2
I0524 15:47:27.979861   40902 certs.go:190] acquiring lock for shared ca certs: {Name:mk94dcea3dce30585f252d5994572acbd0d16248 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 15:47:27.980349   40902 certs.go:199] skipping minikubeCA CA generation: /Users/michaelshechter/.minikube/ca.key
I0524 15:47:27.980524   40902 certs.go:199] skipping proxyClientCA CA generation: /Users/michaelshechter/.minikube/proxy-client-ca.key
I0524 15:47:27.980742   40902 certs.go:315] skipping minikube-user signed cert generation: /Users/michaelshechter/.minikube/profiles/minikube/client.key
I0524 15:47:27.981254   40902 certs.go:315] skipping minikube signed cert generation: /Users/michaelshechter/.minikube/profiles/minikube/apiserver.key.cee25041
I0524 15:47:27.981592   40902 certs.go:315] skipping aggregator signed cert generation: /Users/michaelshechter/.minikube/profiles/minikube/proxy-client.key
I0524 15:47:27.981966   40902 certs.go:437] found cert: /Users/michaelshechter/.minikube/certs/Users/michaelshechter/.minikube/certs/ca-key.pem (1675 bytes)
I0524 15:47:27.982003   40902 certs.go:437] found cert: /Users/michaelshechter/.minikube/certs/Users/michaelshechter/.minikube/certs/ca.pem (1103 bytes)
I0524 15:47:27.982026   40902 certs.go:437] found cert: /Users/michaelshechter/.minikube/certs/Users/michaelshechter/.minikube/certs/cert.pem (1147 bytes)
I0524 15:47:27.982059   40902 certs.go:437] found cert: /Users/michaelshechter/.minikube/certs/Users/michaelshechter/.minikube/certs/key.pem (1679 bytes)
I0524 15:47:27.983688   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0524 15:47:27.996418   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0524 15:47:28.009111   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0524 15:47:28.022638   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0524 15:47:28.036312   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0524 15:47:28.049501   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0524 15:47:28.063210   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0524 15:47:28.075681   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0524 15:47:28.088307   40902 ssh_runner.go:362] scp /Users/michaelshechter/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0524 15:47:28.101439   40902 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0524 15:47:28.113162   40902 ssh_runner.go:195] Run: openssl version
I0524 15:47:28.118980   40902 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0524 15:47:28.125835   40902 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0524 15:47:28.128288   40902 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb  6 18:51 /usr/share/ca-certificates/minikubeCA.pem
I0524 15:47:28.128442   40902 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0524 15:47:28.132646   40902 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0524 15:47:28.139011   40902 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0524 15:47:28.141771   40902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0524 15:47:28.146433   40902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0524 15:47:28.150828   40902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0524 15:47:28.156087   40902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0524 15:47:28.160200   40902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0524 15:47:28.164321   40902 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0524 15:47:28.168749   40902 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:7902 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0524 15:47:28.169020   40902 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0524 15:47:28.181057   40902 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0524 15:47:28.186442   40902 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0524 15:47:28.186470   40902 kubeadm.go:636] restartCluster start
I0524 15:47:28.186599   40902 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0524 15:47:28.191400   40902 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0524 15:47:28.191582   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0524 15:47:28.242721   40902 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:50007"
I0524 15:47:28.242747   40902 kubeconfig.go:135] verify returned: got: 127.0.0.1:50007, want: 127.0.0.1:58455
I0524 15:47:28.243765   40902 lock.go:35] WriteFile acquiring /Users/michaelshechter/.kube/config: {Name:mk776d884df66d45d749003ac0bc92a1fd1ea11b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 15:47:28.250058   40902 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0524 15:47:28.256250   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:28.256343   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:28.262903   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:28.262910   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:28.262997   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:28.268291   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:28.770587   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:28.770806   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:28.779703   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:29.268567   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:29.268942   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:29.288753   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:29.769082   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:29.769853   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:29.800301   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:30.268674   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:30.269554   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:30.298526   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:30.769527   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:30.770292   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:30.800604   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:31.268869   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:31.269139   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:31.289773   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:31.769434   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:31.769813   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:31.794928   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:32.269463   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:32.270069   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:32.289300   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:32.768484   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:32.768887   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:32.789088   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:33.268944   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:33.269284   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:33.286908   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:33.769063   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:33.769702   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:33.800637   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:34.269460   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:34.270065   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:34.302656   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:34.769626   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:34.769985   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:34.788243   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:35.269353   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:35.269574   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:35.287535   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:35.769441   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:35.770132   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:35.818226   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:36.268640   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:36.268783   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:36.275547   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:36.769066   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:36.769455   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:36.786952   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:37.269426   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:37.269865   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:37.290891   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:37.768655   40902 api_server.go:166] Checking apiserver status ...
I0524 15:47:37.768871   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 15:47:37.798056   40902 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0524 15:47:38.258497   40902 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0524 15:47:38.258554   40902 kubeadm.go:1128] stopping kube-system containers ...
I0524 15:47:38.259282   40902 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0524 15:47:38.298877   40902 docker.go:469] Stopping containers: [958ea4c06cd0 41a1257c3b71 ab1e7ab273cd ff14883828c6 a9b40e6b9df5 0194ae816d18 88df5e3962cd eb824310f0d7 72010740c32c ae6b3be8164a ed41ab707aa8 b40f5adc3835 cb710eea880c d0dbed3b967d 65780547c5ba 639eeec894cb 4fb09ec71f78 e20b6c6fedc7 2e89ca7836e5 8f9a259ac51d 25b23480d1c9 fbfc1f8bcec1 512ba8cdb637 227e773f1276 3a1a9849c50b 597ebbb749d3 e8f5e3cdcabe]
I0524 15:47:38.299274   40902 ssh_runner.go:195] Run: docker stop 958ea4c06cd0 41a1257c3b71 ab1e7ab273cd ff14883828c6 a9b40e6b9df5 0194ae816d18 88df5e3962cd eb824310f0d7 72010740c32c ae6b3be8164a ed41ab707aa8 b40f5adc3835 cb710eea880c d0dbed3b967d 65780547c5ba 639eeec894cb 4fb09ec71f78 e20b6c6fedc7 2e89ca7836e5 8f9a259ac51d 25b23480d1c9 fbfc1f8bcec1 512ba8cdb637 227e773f1276 3a1a9849c50b 597ebbb749d3 e8f5e3cdcabe
I0524 15:47:38.321019   40902 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0524 15:47:38.333283   40902 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0524 15:47:38.338771   40902 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Feb 15 19:50 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Mar  3 14:52 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5655 Feb 15 19:50 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Mar  3 14:52 /etc/kubernetes/scheduler.conf

I0524 15:47:38.338938   40902 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0524 15:47:38.345179   40902 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0524 15:47:38.350665   40902 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0524 15:47:38.356032   40902 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0524 15:47:38.356189   40902 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0524 15:47:38.362852   40902 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0524 15:47:38.367906   40902 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0524 15:47:38.368100   40902 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0524 15:47:38.373172   40902 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0524 15:47:38.378764   40902 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0524 15:47:38.378772   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0524 15:47:38.485184   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0524 15:47:39.246293   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0524 15:47:39.345228   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0524 15:47:39.378242   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0524 15:47:39.411520   40902 api_server.go:52] waiting for apiserver process to appear ...
I0524 15:47:39.411844   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 15:47:39.418720   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 15:47:39.928995   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 15:47:40.429049   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 15:47:40.929841   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 15:47:40.957414   40902 api_server.go:72] duration metric: took 1.545894958s to wait for apiserver process to appear ...
I0524 15:47:40.957445   40902 api_server.go:88] waiting for apiserver healthz status ...
I0524 15:47:40.957710   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:40.959295   40902 api_server.go:269] stopped: https://127.0.0.1:58455/healthz: Get "https://127.0.0.1:58455/healthz": EOF
I0524 15:47:40.959319   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:40.960346   40902 api_server.go:269] stopped: https://127.0.0.1:58455/healthz: Get "https://127.0.0.1:58455/healthz": EOF
I0524 15:47:41.463318   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:43.398258   40902 api_server.go:279] https://127.0.0.1:58455/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0524 15:47:43.398275   40902 api_server.go:103] status: https://127.0.0.1:58455/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0524 15:47:43.398286   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:43.464083   40902 api_server.go:279] https://127.0.0.1:58455/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0524 15:47:43.464115   40902 api_server.go:103] status: https://127.0.0.1:58455/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0524 15:47:43.464127   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:43.474960   40902 api_server.go:279] https://127.0.0.1:58455/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0524 15:47:43.474982   40902 api_server.go:103] status: https://127.0.0.1:58455/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0524 15:47:43.960992   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:43.971363   40902 api_server.go:279] https://127.0.0.1:58455/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0524 15:47:43.971392   40902 api_server.go:103] status: https://127.0.0.1:58455/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0524 15:47:44.461219   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:44.467633   40902 api_server.go:279] https://127.0.0.1:58455/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0524 15:47:44.467652   40902 api_server.go:103] status: https://127.0.0.1:58455/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0524 15:47:44.961193   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:44.969002   40902 api_server.go:279] https://127.0.0.1:58455/healthz returned 200:
ok
I0524 15:47:44.979552   40902 api_server.go:141] control plane version: v1.28.3
I0524 15:47:44.979570   40902 api_server.go:131] duration metric: took 4.022144s to wait for apiserver health ...
I0524 15:47:44.979575   40902 cni.go:84] Creating CNI manager for ""
I0524 15:47:44.979584   40902 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0524 15:47:44.989977   40902 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0524 15:47:44.995757   40902 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0524 15:47:45.003923   40902 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0524 15:47:45.017528   40902 system_pods.go:43] waiting for kube-system pods to appear ...
I0524 15:47:45.028474   40902 system_pods.go:59] 7 kube-system pods found
I0524 15:47:45.028489   40902 system_pods.go:61] "coredns-5dd5756b68-bfcgm" [a4d5066a-1c67-4a0d-a85f-63ba46e7b7ff] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0524 15:47:45.028493   40902 system_pods.go:61] "etcd-minikube" [c8779d0b-5dfc-491c-aee3-30bc335030c2] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0524 15:47:45.028497   40902 system_pods.go:61] "kube-apiserver-minikube" [2596ef20-2787-4474-9883-6aa4de94f3ab] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0524 15:47:45.028500   40902 system_pods.go:61] "kube-controller-manager-minikube" [7fbe41f8-ca99-4aa5-9dc4-1908e30f218f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0524 15:47:45.028503   40902 system_pods.go:61] "kube-proxy-r65nv" [00ad8c67-aa33-4311-83ff-72b8365f9a68] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0524 15:47:45.028506   40902 system_pods.go:61] "kube-scheduler-minikube" [1860388e-b0de-4019-bbd3-d90a07c98496] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0524 15:47:45.028511   40902 system_pods.go:61] "storage-provisioner" [b9a61ce9-3ab2-4919-850e-9f0e16525a4d] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0524 15:47:45.028514   40902 system_pods.go:74] duration metric: took 10.955334ms to wait for pod list to return data ...
I0524 15:47:45.028518   40902 node_conditions.go:102] verifying NodePressure condition ...
I0524 15:47:45.051266   40902 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0524 15:47:45.051284   40902 node_conditions.go:123] node cpu capacity is 10
I0524 15:47:45.051296   40902 node_conditions.go:105] duration metric: took 22.773917ms to run NodePressure ...
I0524 15:47:45.051314   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0524 15:47:45.280328   40902 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0524 15:47:45.356341   40902 ops.go:34] apiserver oom_adj: -16
I0524 15:47:45.356364   40902 kubeadm.go:640] restartCluster took 17.169992458s
I0524 15:47:45.356388   40902 kubeadm.go:406] StartCluster complete in 17.187756333s
I0524 15:47:45.356413   40902 settings.go:142] acquiring lock: {Name:mkd099f7232d8af39291f6dddadc940a12cfd163 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 15:47:45.358882   40902 settings.go:150] Updating kubeconfig:  /Users/michaelshechter/.kube/config
I0524 15:47:45.362815   40902 lock.go:35] WriteFile acquiring /Users/michaelshechter/.kube/config: {Name:mk776d884df66d45d749003ac0bc92a1fd1ea11b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 15:47:45.363697   40902 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0524 15:47:45.364017   40902 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0524 15:47:45.363888   40902 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0524 15:47:45.364369   40902 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0524 15:47:45.364382   40902 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0524 15:47:45.364385   40902 addons.go:240] addon storage-provisioner should already be in state true
I0524 15:47:45.364396   40902 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0524 15:47:45.364416   40902 addons.go:69] Setting dashboard=true in profile "minikube"
I0524 15:47:45.364428   40902 addons.go:231] Setting addon dashboard=true in "minikube"
W0524 15:47:45.364434   40902 addons.go:240] addon dashboard should already be in state true
I0524 15:47:45.364473   40902 addons.go:69] Setting ingress=true in profile "minikube"
I0524 15:47:45.364533   40902 addons.go:231] Setting addon ingress=true in "minikube"
W0524 15:47:45.364538   40902 addons.go:240] addon ingress should already be in state true
I0524 15:47:45.364634   40902 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0524 15:47:45.364949   40902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 15:47:45.365085   40902 host.go:66] Checking if "minikube" exists ...
I0524 15:47:45.365092   40902 host.go:66] Checking if "minikube" exists ...
I0524 15:47:45.365156   40902 host.go:66] Checking if "minikube" exists ...
I0524 15:47:45.365516   40902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 15:47:45.365527   40902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 15:47:45.365595   40902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 15:47:45.373689   40902 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0524 15:47:45.373720   40902 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0524 15:47:45.391294   40902 out.go:177] 🔎  Verifying Kubernetes components...
I0524 15:47:45.402377   40902 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0524 15:47:45.444997   40902 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0524 15:47:45.449347   40902 out.go:177] 💡  After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0524 15:47:45.441180   40902 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0524 15:47:45.462693   40902 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
W0524 15:47:45.462731   40902 addons.go:240] addon default-storageclass should already be in state true
I0524 15:47:45.471433   40902 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0524 15:47:45.471534   40902 host.go:66] Checking if "minikube" exists ...
I0524 15:47:45.478467   40902 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0524 15:47:45.485707   40902 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0524 15:47:45.495391   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0524 15:47:45.495399   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0524 15:47:45.495403   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0524 15:47:45.486311   40902 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 15:47:45.495448   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:45.495450   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:45.500468   40902 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/controller:v1.9.4
I0524 15:47:45.510994   40902 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0524 15:47:45.515665   40902 addons.go:423] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0524 15:47:45.515672   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16103 bytes)
I0524 15:47:45.515778   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:45.546451   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:45.546541   40902 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0524 15:47:45.546552   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0524 15:47:45.546627   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:45.546641   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 15:47:45.578765   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:45.601938   40902 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58456 SSHKeyPath:/Users/michaelshechter/.minikube/machines/minikube/id_rsa Username:docker}
I0524 15:47:45.949925   40902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0524 15:47:45.950761   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0524 15:47:45.950781   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0524 15:47:45.951907   40902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0524 15:47:46.049522   40902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0524 15:47:46.067390   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0524 15:47:46.067415   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0524 15:47:46.249738   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0524 15:47:46.249757   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0524 15:47:46.349668   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0524 15:47:46.349692   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0524 15:47:46.369681   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0524 15:47:46.369698   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0524 15:47:46.467891   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0524 15:47:46.467908   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0524 15:47:46.564815   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0524 15:47:46.564830   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0524 15:47:46.668362   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0524 15:47:46.668377   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0524 15:47:46.764344   40902 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0524 15:47:46.764368   40902 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0524 15:47:46.864128   40902 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0524 15:47:47.660969   40902 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (2.297248584s)
I0524 15:47:47.661076   40902 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0524 15:47:47.661204   40902 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (2.258824625s)
I0524 15:47:47.661410   40902 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0524 15:47:47.716461   40902 api_server.go:52] waiting for apiserver process to appear ...
I0524 15:47:47.716624   40902 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 15:47:50.068472   40902 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (4.118530458s)
I0524 15:47:51.064067   40902 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.112130625s)
I0524 15:47:52.056838   40902 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (6.007324417s)
I0524 15:47:52.056980   40902 addons.go:467] Verifying addon ingress=true in "minikube"
I0524 15:47:52.072981   40902 out.go:177] 🔎  Verifying ingress addon...
I0524 15:47:52.057070   40902 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (5.192924s)
I0524 15:47:52.057110   40902 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (4.340503458s)
I0524 15:47:52.086504   40902 api_server.go:72] duration metric: took 6.71279575s to wait for apiserver process to appear ...
I0524 15:47:52.086522   40902 api_server.go:88] waiting for apiserver healthz status ...
I0524 15:47:52.096765   40902 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0524 15:47:52.086535   40902 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58455/healthz ...
I0524 15:47:52.087261   40902 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0524 15:47:52.112612   40902 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0524 15:47:52.112631   40902 kapi.go:107] duration metric: took 25.372ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0524 15:47:52.120156   40902 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner, dashboard, ingress
I0524 15:47:52.115102   40902 api_server.go:279] https://127.0.0.1:58455/healthz returned 200:
ok
I0524 15:47:52.132558   40902 addons.go:502] enable addons completed in 6.768725833s: enabled=[default-storageclass storage-provisioner dashboard ingress]
I0524 15:47:52.121649   40902 api_server.go:141] control plane version: v1.28.3
I0524 15:47:52.132596   40902 api_server.go:131] duration metric: took 46.067791ms to wait for apiserver health ...
I0524 15:47:52.132603   40902 system_pods.go:43] waiting for kube-system pods to appear ...
I0524 15:47:52.154764   40902 system_pods.go:59] 7 kube-system pods found
I0524 15:47:52.154800   40902 system_pods.go:61] "coredns-5dd5756b68-bfcgm" [a4d5066a-1c67-4a0d-a85f-63ba46e7b7ff] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0524 15:47:52.154807   40902 system_pods.go:61] "etcd-minikube" [c8779d0b-5dfc-491c-aee3-30bc335030c2] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0524 15:47:52.154815   40902 system_pods.go:61] "kube-apiserver-minikube" [2596ef20-2787-4474-9883-6aa4de94f3ab] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0524 15:47:52.154830   40902 system_pods.go:61] "kube-controller-manager-minikube" [7fbe41f8-ca99-4aa5-9dc4-1908e30f218f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0524 15:47:52.154835   40902 system_pods.go:61] "kube-proxy-r65nv" [00ad8c67-aa33-4311-83ff-72b8365f9a68] Running
I0524 15:47:52.154842   40902 system_pods.go:61] "kube-scheduler-minikube" [1860388e-b0de-4019-bbd3-d90a07c98496] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0524 15:47:52.154851   40902 system_pods.go:61] "storage-provisioner" [b9a61ce9-3ab2-4919-850e-9f0e16525a4d] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0524 15:47:52.154863   40902 system_pods.go:74] duration metric: took 22.254833ms to wait for pod list to return data ...
I0524 15:47:52.154879   40902 kubeadm.go:581] duration metric: took 6.781181792s to wait for : map[apiserver:true system_pods:true] ...
I0524 15:47:52.154900   40902 node_conditions.go:102] verifying NodePressure condition ...
I0524 15:47:52.164704   40902 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0524 15:47:52.164720   40902 node_conditions.go:123] node cpu capacity is 10
I0524 15:47:52.164738   40902 node_conditions.go:105] duration metric: took 9.833375ms to run NodePressure ...
I0524 15:47:52.164750   40902 start.go:228] waiting for startup goroutines ...
I0524 15:47:52.164757   40902 start.go:233] waiting for cluster config update ...
I0524 15:47:52.164771   40902 start.go:242] writing updated cluster config ...
I0524 15:47:52.165384   40902 ssh_runner.go:195] Run: rm -f paused
I0524 15:47:52.387372   40902 start.go:600] kubectl: 1.29.0, cluster: 1.28.3 (minor skew: 1)
I0524 15:47:52.395518   40902 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* May 24 12:47:49 minikube cri-dockerd[1140]: time="2024-05-24T12:47:49Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.10.1: Status: Image is up to date for quay.io/argoproj/argocd:v2.10.1"
May 24 12:47:52 minikube cri-dockerd[1140]: time="2024-05-24T12:47:52Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.10.1: Status: Image is up to date for quay.io/argoproj/argocd:v2.10.1"
May 24 12:47:54 minikube cri-dockerd[1140]: time="2024-05-24T12:47:54Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.10.1: Status: Image is up to date for quay.io/argoproj/argocd:v2.10.1"
May 24 12:47:55 minikube cri-dockerd[1140]: time="2024-05-24T12:47:55Z" level=info msg="Stop pulling image redis:7.0.14-alpine: Status: Image is up to date for redis:7.0.14-alpine"
May 24 12:47:57 minikube cri-dockerd[1140]: time="2024-05-24T12:47:57Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.10.1: Status: Image is up to date for quay.io/argoproj/argocd:v2.10.1"
May 24 12:48:00 minikube cri-dockerd[1140]: time="2024-05-24T12:48:00Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.10.1: Status: Image is up to date for quay.io/argoproj/argocd:v2.10.1"
May 24 12:48:00 minikube dockerd[873]: time="2024-05-24T12:48:00.081375626Z" level=info msg="ignoring event" container=b73ee9d2b18161ae85eeb4d8c8b77c07577c9e630e4bd94556df2934d5827939 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:48:01 minikube dockerd[873]: time="2024-05-24T12:48:01.715108794Z" level=info msg="ignoring event" container=e2f22ba014b82e348479283c4e53018783cc5fe81137ba99d2c1f6236edf7411 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:48:02 minikube cri-dockerd[1140]: time="2024-05-24T12:48:02Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.10.1: Status: Image is up to date for quay.io/argoproj/argocd:v2.10.1"
May 24 12:48:03 minikube cri-dockerd[1140]: time="2024-05-24T12:48:03Z" level=info msg="Stop pulling image ghcr.io/dexidp/dex:v2.37.0: Status: Image is up to date for ghcr.io/dexidp/dex:v2.37.0"
May 24 12:48:05 minikube dockerd[873]: time="2024-05-24T12:48:05.680203712Z" level=info msg="ignoring event" container=b9429e32357b024a62588e849d7a40dfca6f25cb8aa738adb7b91523e15bc229 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:48:29 minikube dockerd[873]: time="2024-05-24T12:48:29.686756418Z" level=info msg="ignoring event" container=fd23ba3ea310c1688b839922e0aa9c2065218b75e8370f8a7c4fe8dcb62f3ae0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:48:34 minikube dockerd[873]: time="2024-05-24T12:48:34.658544295Z" level=info msg="ignoring event" container=0abc046e08aacadda3f27902b27b25e8905b38aa68badfc9dc4889e986f260e2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:49:10 minikube dockerd[873]: time="2024-05-24T12:49:10.704933465Z" level=info msg="ignoring event" container=9fb525dbe90db7e6cba18beb01d8456a755e0e3031f6c81987fc86b43651ffaf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:49:25 minikube dockerd[873]: time="2024-05-24T12:49:25.664322514Z" level=info msg="ignoring event" container=1cb554e486fe54144be6d6cb8090c6f97090252c01f66e26f6f5c8876fb93e8c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:50:41 minikube dockerd[873]: time="2024-05-24T12:50:41.704672132Z" level=info msg="ignoring event" container=8630e309d7b80ba9c9d33bb7a882b40c3c5038a106afd7081fdc1e389af31e6c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:50:51 minikube dockerd[873]: time="2024-05-24T12:50:51.952965762Z" level=info msg="ignoring event" container=396154a5aad092ea0be2d0898a17bfdcad9bcd384ae0665ec35f420176fd6ad9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:53:28 minikube dockerd[873]: time="2024-05-24T12:53:28.682897042Z" level=info msg="ignoring event" container=2eb3874ff2190d41464b88e1ac6aed5e3d2de3d5ebdb139162295bcccd9c5258 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:53:42 minikube dockerd[873]: time="2024-05-24T12:53:42.670181132Z" level=info msg="ignoring event" container=298bb74c9726e147c73e21d07b7b435bac3d9de4e6a47109cb9bb3d6b3bda224 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:58:40 minikube dockerd[873]: time="2024-05-24T12:58:40.688904506Z" level=info msg="ignoring event" container=23ef697e613d970c9868918acd0e1892907eefc82691e70f3bac1950a9ba2df6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:58:50 minikube dockerd[873]: time="2024-05-24T12:58:50.675410261Z" level=info msg="ignoring event" container=c0a398847b3823d1c64e68545371b6f0050de032f5eed185935788535f30d36f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 12:58:51 minikube cri-dockerd[1140]: time="2024-05-24T12:58:51Z" level=error msg="Error response from daemon: No such container: 298bb74c9726e147c73e21d07b7b435bac3d9de4e6a47109cb9bb3d6b3bda224 Failed to get stats from container 298bb74c9726e147c73e21d07b7b435bac3d9de4e6a47109cb9bb3d6b3bda224"
May 24 12:58:51 minikube cri-dockerd[1140]: time="2024-05-24T12:58:51Z" level=error msg="Error response from daemon: No such container: 298bb74c9726e147c73e21d07b7b435bac3d9de4e6a47109cb9bb3d6b3bda224 Failed to get stats from container 298bb74c9726e147c73e21d07b7b435bac3d9de4e6a47109cb9bb3d6b3bda224"
May 24 13:03:46 minikube dockerd[873]: time="2024-05-24T13:03:46.679509593Z" level=info msg="ignoring event" container=cc71b3f09fbbd0c2ee0cf0ba565656b88b3b0a8ed59fcfd09f88af43fe4ece31 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:03:55 minikube dockerd[873]: time="2024-05-24T13:03:55.658088055Z" level=info msg="ignoring event" container=118c64fff3ad9b1fd52570decd10bf2fa6347b088c3aecf69dba25ce7121b4db module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:09:01 minikube dockerd[873]: time="2024-05-24T13:09:01.668739919Z" level=info msg="ignoring event" container=64b98c7a11548cff8fee9f3580502662d84c18be6cb6624a7952272e081fda57 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:09:04 minikube dockerd[873]: time="2024-05-24T13:09:04.657429045Z" level=info msg="ignoring event" container=273994dafa636f2c67d0aec9e70acb25485db46cf88fa03d0196b15600d2f813 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:14:12 minikube dockerd[873]: time="2024-05-24T13:14:12.683370299Z" level=info msg="ignoring event" container=3427231d339ea355adc531fda07e6c83ac96c26f7bfe0ac7dbdb0d6c683d01b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:14:15 minikube dockerd[873]: time="2024-05-24T13:14:15.666666925Z" level=info msg="ignoring event" container=d4fb4764d65cd010563a97986272e2ffb7017f9e5b8cf3de7496e07d7736662c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:17:30 minikube cri-dockerd[1140]: time="2024-05-24T13:17:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f56874c3e8c9247698eb8724e79789e90c930e1d79ad83e2cb8c251b672c16f1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 24 13:17:32 minikube dockerd[873]: time="2024-05-24T13:17:32.296495794Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 24 13:17:32 minikube dockerd[873]: time="2024-05-24T13:17:32.296646502Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 13:17:47 minikube dockerd[873]: time="2024-05-24T13:17:47.499703885Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 24 13:17:47 minikube dockerd[873]: time="2024-05-24T13:17:47.499806135Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 13:18:10 minikube dockerd[873]: time="2024-05-24T13:18:10.508788881Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 24 13:18:10 minikube dockerd[873]: time="2024-05-24T13:18:10.508831381Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 13:19:04 minikube dockerd[873]: time="2024-05-24T13:19:04.514492920Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 24 13:19:04 minikube dockerd[873]: time="2024-05-24T13:19:04.514525920Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 13:19:25 minikube dockerd[873]: time="2024-05-24T13:19:25.628895847Z" level=info msg="ignoring event" container=c28c55758fda96a8c7c823fa22fdb660fafa6faf4f957b358ea7d0d9ed32afde module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:19:26 minikube dockerd[873]: time="2024-05-24T13:19:26.626545625Z" level=info msg="ignoring event" container=969ce3bd0e3f565a761dfc60c42a69366fc669cc42f5c99a84d757a265c7954a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:20:27 minikube dockerd[873]: time="2024-05-24T13:20:27.254563667Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 24 13:20:27 minikube dockerd[873]: time="2024-05-24T13:20:27.254641417Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 13:22:44 minikube dockerd[873]: time="2024-05-24T13:22:44.365005217Z" level=info msg="ignoring event" container=0ce63cb2e02ef472eed2cf887bbd559caa6b2047a0d7569d41910c59ee0a60d9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:22:44 minikube dockerd[873]: time="2024-05-24T13:22:44.365158008Z" level=info msg="ignoring event" container=bb78f8c2567cb886d947a520a5525b2d0dba40166401b3b2150bd2d31d8f642b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:22:44 minikube dockerd[873]: time="2024-05-24T13:22:44.470880592Z" level=info msg="ignoring event" container=4e27c945d55de8992b70d999360c6088909931b62b77f5808d922c989e4a2d45 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:22:45 minikube cri-dockerd[1140]: time="2024-05-24T13:22:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c1fd2fd88794df0a56e61930a21e9e74a3a9fa1ce0c47d46113c50551a78b8d0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 24 13:22:45 minikube dockerd[873]: time="2024-05-24T13:22:45.633940759Z" level=info msg="ignoring event" container=ef05f547470e955b75378efc9e9946dac3df7ba41b40b9ffb4c0bc124dfc14db module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:23:14 minikube dockerd[873]: time="2024-05-24T13:23:14.532846133Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 24 13:23:14 minikube dockerd[873]: time="2024-05-24T13:23:14.532882883Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 13:23:28 minikube dockerd[873]: time="2024-05-24T13:23:28.954608709Z" level=info msg="ignoring event" container=df2a188ea133eea329e15b7cf6bc8f1bb2bbf738accce20e0a4d1915442a0390 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:23:29 minikube dockerd[873]: time="2024-05-24T13:23:29.065810168Z" level=info msg="ignoring event" container=1dfe055f6322855b563279efa693b489e6aada44822df6160ba9ad23aae61938 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:23:38 minikube dockerd[873]: time="2024-05-24T13:23:38.322676089Z" level=info msg="ignoring event" container=3f58543ef4043bc5432f8e4c2cc0f2e5cc2adf2601be65e125004386d7fbf345 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:23:38 minikube dockerd[873]: time="2024-05-24T13:23:38.417524005Z" level=info msg="ignoring event" container=6ac9fa8eabe9d166ef77e93ea49863c8d4e3ee007d98d2bd61ba7d1841e387ae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:24:29 minikube dockerd[873]: time="2024-05-24T13:24:29.641064334Z" level=info msg="ignoring event" container=1fc787927754e81a51e055661f4f267a11c3497449e36c561df770233afac17f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:24:35 minikube dockerd[873]: time="2024-05-24T13:24:35.645947879Z" level=info msg="ignoring event" container=918d04043c469f48229bbe9821b369b6c0e95ba013a6f5c5370bbedd3dd748e0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:28:19 minikube dockerd[873]: time="2024-05-24T13:28:19.608581594Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 24 13:28:19 minikube dockerd[873]: time="2024-05-24T13:28:19.608892302Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 13:29:36 minikube dockerd[873]: time="2024-05-24T13:29:36.677733379Z" level=info msg="ignoring event" container=a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 13:29:37 minikube cri-dockerd[1140]: time="2024-05-24T13:29:37Z" level=error msg="Error response from daemon: No such container: 918d04043c469f48229bbe9821b369b6c0e95ba013a6f5c5370bbedd3dd748e0 Failed to get stats from container 918d04043c469f48229bbe9821b369b6c0e95ba013a6f5c5370bbedd3dd748e0"
May 24 13:29:37 minikube dockerd[873]: time="2024-05-24T13:29:37.673532172Z" level=info msg="ignoring event" container=c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                               ATTEMPT             POD ID              POD
c56a0a054a1c9       fc7ff53d1e8fc                                                                                                                2 minutes ago       Exited              myapp                              1865                03601438b4e7c       myapp-55c645d9b5-rj8cc
a3e49ab869017       fc7ff53d1e8fc                                                                                                                2 minutes ago       Exited              myapp                              1873                ef46c5e802490       myapp-55c645d9b5-k8g5l
d266e325abcb5       a112d678f1ebb                                                                                                                9 minutes ago       Running             config-reloader                    0                   c1fd2fd88794d       alertmanager-prom-example-kube-promethe-alertmanager-0
5c34c44436824       654a649b6b2f3                                                                                                                9 minutes ago       Running             alertmanager                       0                   c1fd2fd88794d       alertmanager-prom-example-kube-promethe-alertmanager-0
ef05f547470e9       a112d678f1ebb                                                                                                                9 minutes ago       Exited              init-config-reloader               0                   c1fd2fd88794d       alertmanager-prom-example-kube-promethe-alertmanager-0
3b9e29c29d5ef       ba04bb24b9575                                                                                                                44 minutes ago      Running             storage-provisioner                194                 bacfa1a2d8aa6       storage-provisioner
dd9fed1630ab8       ghcr.io/dexidp/dex@sha256:f579d00721b0d842328c43a562f50343c54b0048ef2d58d6b54e750c21fc7938                                   44 minutes ago      Running             dex                                26                  5a3637491e1d5       argocd-dex-server-5fb9857f6c-mrzn4
fecd796472b4c       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              44 minutes ago      Running             argocd-repo-server                 68                  4a980b755ec42       argocd-repo-server-554bd7bf5-7nsfk
e63f08286d22f       20b332c9a70d8                                                                                                                44 minutes ago      Running             kubernetes-dashboard               36                  963b404b35f4a       kubernetes-dashboard-8694d4445c-msdbf
b73ee9d2b1816       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              44 minutes ago      Exited              copyutil                           4                   5a3637491e1d5       argocd-dex-server-5fb9857f6c-mrzn4
75cfda2908d17       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              44 minutes ago      Running             argocd-application-controller      157                 5c65543233c07       argocd-application-controller-0
1c76db392dc12       redis@sha256:45de526e9fbc1a4b183957ab93a448294181fae10ced9184fc6efe9956ca0ccc                                                44 minutes ago      Running             redis                              4                   0d23f7bd38a51       argocd-redis-66d9777b78-dsv79
338093d4b5ea1       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              44 minutes ago      Running             argocd-server                      76                  ab930375ebaa9       argocd-server-6697c5749b-dwl8z
79fd4e5669c4f       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              44 minutes ago      Running             argocd-applicationset-controller   36                  48a4319052a18       argocd-applicationset-controller-64c77cff5d-7vf7g
911d7adc81bf4       a112d678f1ebb                                                                                                                44 minutes ago      Running             config-reloader                    19                  a0fe350e1b99d       prometheus-prom-example-kube-promethe-prometheus-0
d55048b553e45       b3165e6ee12be                                                                                                                44 minutes ago      Running             prometheus                         207                 a0fe350e1b99d       prometheus-prom-example-kube-promethe-prometheus-0
c74d4c480c4c3       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              44 minutes ago      Running             argocd-notifications-controller    68                  0e6ad1ae61cb9       argocd-notifications-controller-679d59bf4b-gn8qc
7a164095cf9b8       fa8de64417ca6                                                                                                                44 minutes ago      Exited              copyutil                           4                   4a980b755ec42       argocd-repo-server-554bd7bf5-7nsfk
c0677283e8ee8       a112d678f1ebb                                                                                                                44 minutes ago      Exited              init-config-reloader               4                   a0fe350e1b99d       prometheus-prom-example-kube-promethe-prometheus-0
496b21d8a575e       97e04611ad434                                                                                                                44 minutes ago      Running             coredns                            13                  ee161979879ff       coredns-5dd5756b68-bfcgm
a0e8f6b19d93b       f065bfef03d73                                                                                                                44 minutes ago      Running             controller                         101                 179009305a1ed       ingress-nginx-controller-7c6974c4d8-cq6qb
907ea034c1441       a422e0e982356                                                                                                                44 minutes ago      Running             dashboard-metrics-scraper          7                   a1f1fb2eedc40       dashboard-metrics-scraper-7fd5cb4ddc-4zxkv
765439c370e19       b7ce331e7cd82                                                                                                                44 minutes ago      Running             node-exporter                      137                 caf8379fb649f       prom-example-prometheus-node-exporter-7z7dd
4c5a62deb665d       20b332c9a70d8                                                                                                                44 minutes ago      Exited              kubernetes-dashboard               35                  963b404b35f4a       kubernetes-dashboard-8694d4445c-msdbf
a20e92ec140ce       a5dd5cdd6d3ef                                                                                                                44 minutes ago      Running             kube-proxy                         7                   0d79ac8d00081       kube-proxy-r65nv
229e259de0568       ba04bb24b9575                                                                                                                44 minutes ago      Exited              storage-provisioner                193                 bacfa1a2d8aa6       storage-provisioner
fc6cc257f5c2e       537e9a59ee2fd                                                                                                                44 minutes ago      Running             kube-apiserver                     19                  eed1249152802       kube-apiserver-minikube
dd50970342603       9cdd6470f48c8                                                                                                                44 minutes ago      Running             etcd                               5                   750aaf0296f9a       etcd-minikube
5d13c3fcab636       8276439b4f237                                                                                                                44 minutes ago      Running             kube-controller-manager            7                   246a506f8bcc6       kube-controller-manager-minikube
7c3b192a134ec       42a4e73724daa                                                                                                                44 minutes ago      Running             kube-scheduler                     7                   26ced40368c4a       kube-scheduler-minikube
1d1f470d23a34       ghcr.io/dexidp/dex@sha256:f579d00721b0d842328c43a562f50343c54b0048ef2d58d6b54e750c21fc7938                                   2 months ago        Exited              dex                                25                  b832d16e44f79       argocd-dex-server-5fb9857f6c-mrzn4
160a5bef1402f       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              2 months ago        Exited              argocd-repo-server                 67                  48794b4e47e6d       argocd-repo-server-554bd7bf5-7nsfk
c5f0cf91ec7cd       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              2 months ago        Exited              argocd-applicationset-controller   35                  be7d6e2a29e41       argocd-applicationset-controller-64c77cff5d-7vf7g
50dcbe1eab1e7       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              2 months ago        Exited              argocd-notifications-controller    67                  d2e3c91544a80       argocd-notifications-controller-679d59bf4b-gn8qc
5717736c2e1bb       redis@sha256:45de526e9fbc1a4b183957ab93a448294181fae10ced9184fc6efe9956ca0ccc                                                2 months ago        Exited              redis                              3                   8f7564e7461a6       argocd-redis-66d9777b78-dsv79
5d1edf3b27a0a       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              2 months ago        Exited              argocd-server                      75                  a314d101abba2       argocd-server-6697c5749b-dwl8z
76c342a0c995a       quay.io/argoproj/argocd@sha256:5f1de1b4d959868c1e006e08d46361c8f019d9730e74bc1feeab8c7b413f1187                              2 months ago        Exited              argocd-application-controller      156                 59e83c7dcccee       argocd-application-controller-0
47caffa464969       a112d678f1ebb                                                                                                                2 months ago        Exited              config-reloader                    18                  594b3a32a031c       prometheus-prom-example-kube-promethe-prometheus-0
cdb84ba1f52ba       b3165e6ee12be                                                                                                                2 months ago        Exited              prometheus                         206                 594b3a32a031c       prometheus-prom-example-kube-promethe-prometheus-0
58ebb37daea43       f065bfef03d73                                                                                                                2 months ago        Exited              controller                         100                 46fe74f087ddf       ingress-nginx-controller-7c6974c4d8-cq6qb
cd78e4a2f70dc       a422e0e982356                                                                                                                2 months ago        Exited              dashboard-metrics-scraper          6                   bb0ff96f49453       dashboard-metrics-scraper-7fd5cb4ddc-4zxkv
ab1e7ab273cdb       97e04611ad434                                                                                                                2 months ago        Exited              coredns                            12                  a9b40e6b9df59       coredns-5dd5756b68-bfcgm
27d9d24744a13       b7ce331e7cd82                                                                                                                2 months ago        Exited              node-exporter                      136                 b4b906b85ac8a       prom-example-prometheus-node-exporter-7z7dd
ff14883828c62       a5dd5cdd6d3ef                                                                                                                2 months ago        Exited              kube-proxy                         6                   88df5e3962cd4       kube-proxy-r65nv
eb824310f0d7f       8276439b4f237                                                                                                                2 months ago        Exited              kube-controller-manager            6                   b40f5adc3835f       kube-controller-manager-minikube
72010740c32c7       537e9a59ee2fd                                                                                                                2 months ago        Exited              kube-apiserver                     18                  cb710eea880cc       kube-apiserver-minikube
ae6b3be8164ad       42a4e73724daa                                                                                                                2 months ago        Exited              kube-scheduler                     6                   d0dbed3b967de       kube-scheduler-minikube
ed41ab707aa8d       9cdd6470f48c8                                                                                                                2 months ago        Exited              etcd                               4                   65780547c5ba0       etcd-minikube
40746efa6dfe2       af594c6a879f2                                                                                                                3 months ago        Exited              patch                              1                   aae6a04854d68       ingress-nginx-admission-patch-7vzbt
060cd4e0ca112       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80   3 months ago        Exited              create                             0                   dc577cdedd3eb       ingress-nginx-admission-create-jcnsx

* 
* ==> controller_ingress [58ebb37daea4] <==
* -------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------

W0303 14:52:30.995743       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0303 14:52:30.995905       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0303 14:52:31.009292       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/arm64"
I0303 14:52:31.987956       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0303 14:52:32.087302       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0303 14:52:32.097933       7 nginx.go:260] "Starting NGINX Ingress controller"
I0303 14:52:32.193016       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"2533c663-e0f4-45d9-9df3-57245f3070e9", APIVersion:"v1", ResourceVersion:"218466", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0303 14:52:32.195256       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"2528a8e6-cad9-4388-be5a-86d5f2dae21b", APIVersion:"v1", ResourceVersion:"218467", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0303 14:52:32.195281       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"0870acf1-0001-4b95-a1f0-9437e52f3036", APIVersion:"v1", ResourceVersion:"218468", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0303 14:52:33.301085       7 nginx.go:303] "Starting NGINX process"
I0303 14:52:33.301169       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0303 14:52:33.301452       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0303 14:52:33.302179       7 controller.go:190] "Configuration changes detected, backend reload required"
I0303 14:52:33.306598       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0303 14:52:33.306708       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-cq6qb"
I0303 14:52:33.309213       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-cq6qb" node="minikube"
I0303 14:52:33.510070       7 controller.go:210] "Backend successfully reloaded"
I0303 14:52:33.510125       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0303 14:52:33.510184       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-cq6qb", UID:"e2ba4b3a-2081-4795-aba0-ce129fd0df98", APIVersion:"v1", ResourceVersion:"647795", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0308 10:03:14.911271       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0308 10:03:14.916233       7 nginx.go:379] "Shutting down controller queues"

* 
* ==> controller_ingress [a0e8f6b19d93] <==
* -------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------

W0524 12:47:48.872212       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0524 12:47:48.872699       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0524 12:47:50.055941       7 main.go:246] Initial connection to the Kubernetes API server was retried 1 times.
I0524 12:47:50.055976       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/arm64"
I0524 12:47:50.854882       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0524 12:47:50.955664       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0524 12:47:51.063892       7 nginx.go:260] "Starting NGINX Ingress controller"
I0524 12:47:51.254038       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"2533c663-e0f4-45d9-9df3-57245f3070e9", APIVersion:"v1", ResourceVersion:"218466", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0524 12:47:51.256777       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"2528a8e6-cad9-4388-be5a-86d5f2dae21b", APIVersion:"v1", ResourceVersion:"218467", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0524 12:47:51.256907       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"0870acf1-0001-4b95-a1f0-9437e52f3036", APIVersion:"v1", ResourceVersion:"218468", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0524 12:47:52.367620       7 nginx.go:303] "Starting NGINX process"
I0524 12:47:52.368542       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0524 12:47:52.368633       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0524 12:47:52.369953       7 controller.go:190] "Configuration changes detected, backend reload required"
I0524 12:47:52.374971       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0524 12:47:52.375131       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-cq6qb"
I0524 12:47:52.378706       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-cq6qb" node="minikube"
I0524 12:47:52.548885       7 controller.go:210] "Backend successfully reloaded"
I0524 12:47:52.548980       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0524 12:47:52.549058       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-cq6qb", UID:"e2ba4b3a-2081-4795-aba0-ce129fd0df98", APIVersion:"v1", ResourceVersion:"740313", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration

* 
* ==> coredns [496b21d8a575] <==
* [INFO] 10.244.0.117:34350 - 30940 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000956666s
[INFO] 10.244.0.117:53236 - 39715 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001405125s
[INFO] 10.244.0.121:34038 - 23152 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000151417s
[INFO] 10.244.0.121:53608 - 27446 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000170166s
[INFO] 10.244.0.121:54111 - 40442 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000070875s
[INFO] 10.244.0.121:46248 - 6972 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000051542s
[INFO] 10.244.0.121:36559 - 29324 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000111875s
[INFO] 10.244.0.121:33491 - 36466 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000057541s
[INFO] 10.244.0.121:59838 - 20860 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.013733917s
[INFO] 10.244.0.121:51947 - 51206 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.013782125s
[INFO] 10.244.0.117:50181 - 7963 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001046625s
[INFO] 10.244.0.117:48331 - 53612 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.00115175s
[INFO] 10.244.0.117:46229 - 54840 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000735708s
[INFO] 10.244.0.117:56566 - 27915 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000184833s
[INFO] 10.244.0.121:36155 - 56131 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000429417s
[INFO] 10.244.0.121:54251 - 38344 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000112417s
[INFO] 10.244.0.121:53511 - 35396 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000400541s
[INFO] 10.244.0.121:52923 - 5092 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000479167s
[INFO] 10.244.0.121:56766 - 44252 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000134708s
[INFO] 10.244.0.121:37560 - 65120 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000157s
[INFO] 10.244.0.121:55818 - 43761 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.019739042s
[INFO] 10.244.0.121:59229 - 58448 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.01981525s
[INFO] 10.244.0.117:34726 - 9970 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000375583s
[INFO] 10.244.0.117:52751 - 16814 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.0003645s
[INFO] 10.244.0.117:48717 - 4678 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001061959s
[INFO] 10.244.0.117:40468 - 15219 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000187167s
[INFO] 10.244.0.121:55049 - 22160 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000313334s
[INFO] 10.244.0.121:55566 - 40319 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000269916s
[INFO] 10.244.0.121:46803 - 35071 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000139s
[INFO] 10.244.0.121:58773 - 22205 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000195917s
[INFO] 10.244.0.121:56165 - 53907 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000208041s
[INFO] 10.244.0.121:37764 - 54060 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000210417s
[INFO] 10.244.0.121:55502 - 13644 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.016574708s
[INFO] 10.244.0.121:46637 - 13921 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.01672625s
[INFO] 10.244.0.117:43475 - 9442 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000364667s
[INFO] 10.244.0.117:56194 - 42369 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000458666s
[INFO] 10.244.0.117:55738 - 44171 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000189708s
[INFO] 10.244.0.117:33778 - 27406 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000241708s
[INFO] 10.244.0.121:57916 - 33050 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000122792s
[INFO] 10.244.0.121:36942 - 54999 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000141375s
[INFO] 10.244.0.121:59548 - 38480 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000086667s
[INFO] 10.244.0.121:51400 - 29974 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.00010225s
[INFO] 10.244.0.121:58227 - 2618 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000081417s
[INFO] 10.244.0.121:32841 - 45689 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000079083s
[INFO] 10.244.0.121:54431 - 52999 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.008739542s
[INFO] 10.244.0.121:53329 - 26152 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.008838167s
[INFO] 10.244.0.117:34308 - 12970 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001168083s
[INFO] 10.244.0.117:41657 - 9589 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001319375s
[INFO] 10.244.0.117:42007 - 31141 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001089791s
[INFO] 10.244.0.117:35647 - 16561 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001187083s
[INFO] 10.244.0.121:47814 - 30241 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000334209s
[INFO] 10.244.0.121:52810 - 23192 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000458208s
[INFO] 10.244.0.121:39172 - 28112 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000147333s
[INFO] 10.244.0.121:45460 - 29872 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000237958s
[INFO] 10.244.0.121:40974 - 19011 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000122667s
[INFO] 10.244.0.121:57553 - 18419 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000122125s
[INFO] 10.244.0.121:51258 - 4846 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.011102791s
[INFO] 10.244.0.121:44329 - 65400 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.014189125s
[INFO] 10.244.0.117:39826 - 22229 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000802792s
[INFO] 10.244.0.117:52827 - 51039 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000905375s

* 
* ==> coredns [ab1e7ab273cd] <==
* [INFO] 10.244.0.94:60096 - 18207 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001460584s
[INFO] 10.244.0.94:33625 - 32463 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001718166s
[INFO] 10.244.0.100:53688 - 45842 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000421584s
[INFO] 10.244.0.100:48132 - 33488 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.00035675s
[INFO] 10.244.0.100:44315 - 24299 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000172291s
[INFO] 10.244.0.100:46483 - 59456 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000104666s
[INFO] 10.244.0.100:33283 - 57392 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.00006375s
[INFO] 10.244.0.100:50040 - 18697 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000078708s
[INFO] 10.244.0.100:51251 - 23029 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.155052417s
[INFO] 10.244.0.100:36066 - 1370 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.155379083s
[INFO] 10.244.0.94:33974 - 49795 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001807417s
[INFO] 10.244.0.94:60575 - 44075 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000219292s
[INFO] 10.244.0.94:60449 - 24272 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001589s
[INFO] 10.244.0.94:39147 - 38331 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001896791s
[INFO] 10.244.0.100:52230 - 32254 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.00042s
[INFO] 10.244.0.100:50469 - 7228 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000583875s
[INFO] 10.244.0.100:49714 - 4154 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000213s
[INFO] 10.244.0.100:46560 - 50795 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000172583s
[INFO] 10.244.0.100:36418 - 23214 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000194958s
[INFO] 10.244.0.100:53587 - 38914 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000136958s
[INFO] 10.244.0.100:42610 - 34543 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.010378959s
[INFO] 10.244.0.100:52301 - 34303 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.010464791s
[INFO] 10.244.0.94:48491 - 23391 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000947208s
[INFO] 10.244.0.94:52134 - 16667 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.0011655s
[INFO] 10.244.0.94:60728 - 14566 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000988875s
[INFO] 10.244.0.94:46002 - 6477 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000206708s
[INFO] 10.244.0.100:44435 - 43451 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000155666s
[INFO] 10.244.0.100:37284 - 25252 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.00038225s
[INFO] 10.244.0.100:52554 - 21869 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000062542s
[INFO] 10.244.0.100:57030 - 55232 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000216791s
[INFO] 10.244.0.100:54031 - 40146 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000048708s
[INFO] 10.244.0.100:37487 - 40287 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000067708s
[INFO] 10.244.0.100:55643 - 56338 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.084180791s
[INFO] 10.244.0.100:44037 - 36329 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.084548s
[INFO] 10.244.0.94:37082 - 40820 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001249084s
[INFO] 10.244.0.94:37755 - 65285 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001472709s
[INFO] 10.244.0.94:51982 - 54150 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001924334s
[INFO] 10.244.0.94:35314 - 58034 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001939208s
[INFO] 10.244.0.100:48649 - 43555 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000167208s
[INFO] 10.244.0.100:37823 - 14039 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000042625s
[INFO] 10.244.0.100:39971 - 52415 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000287042s
[INFO] 10.244.0.100:35174 - 33328 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000133875s
[INFO] 10.244.0.100:40558 - 16890 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000043875s
[INFO] 10.244.0.100:39874 - 37133 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000057625s
[INFO] 10.244.0.100:55875 - 8842 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.063764458s
[INFO] 10.244.0.100:57358 - 11433 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.0640295s
[INFO] 10.244.0.94:37764 - 23084 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001089917s
[INFO] 10.244.0.94:44108 - 21480 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001527375s
[INFO] 10.244.0.94:51213 - 33140 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001070667s
[INFO] 10.244.0.94:51172 - 45425 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000423083s
[INFO] 10.244.0.100:57581 - 17184 "AAAA IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000306584s
[INFO] 10.244.0.100:42555 - 12361 "A IN gitlab.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000447333s
[INFO] 10.244.0.100:43842 - 58946 "A IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.00011925s
[INFO] 10.244.0.100:52632 - 57075 "AAAA IN gitlab.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.0000625s
[INFO] 10.244.0.100:57513 - 42218 "A IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000197334s
[INFO] 10.244.0.100:35171 - 58141 "AAAA IN gitlab.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000062791s
[INFO] 10.244.0.100:59729 - 19446 "AAAA IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 66 0.551312959s
[INFO] 10.244.0.100:45494 - 14603 "A IN gitlab.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.599062667s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_02_06T20_51_09_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 06 Feb 2024 18:51:06 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 24 May 2024 13:32:29 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 24 May 2024 13:28:37 +0000   Sat, 24 Feb 2024 14:55:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 24 May 2024 13:28:37 +0000   Sat, 24 Feb 2024 14:55:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 24 May 2024 13:28:37 +0000   Sat, 24 Feb 2024 14:55:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 24 May 2024 13:28:37 +0000   Sat, 24 Feb 2024 14:55:25 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                10
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8131308Ki
  pods:               110
Allocatable:
  cpu:                10
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8131308Ki
  pods:               110
System Info:
  Machine ID:                 4d49526c765d42dc8caa9c89f4fc55ba
  System UUID:                4d49526c765d42dc8caa9c89f4fc55ba
  Boot ID:                    d85b36ee-dd30-4fae-b8f3-e2e8e9c182b9
  Kernel Version:             6.6.22-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (23 in total)
  Namespace                   Name                                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                      ------------  ----------  ---------------  -------------  ---
  argocd                      argocd-application-controller-0                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  argocd                      argocd-applicationset-controller-64c77cff5d-7vf7g         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  argocd                      argocd-dex-server-5fb9857f6c-mrzn4                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  argocd                      argocd-notifications-controller-679d59bf4b-gn8qc          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  argocd                      argocd-redis-66d9777b78-dsv79                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  argocd                      argocd-repo-server-554bd7bf5-7nsfk                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  argocd                      argocd-server-6697c5749b-dwl8z                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  default                     alertmanager-prom-example-kube-promethe-alertmanager-0    0 (0%!)(MISSING)        0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         9m49s
  default                     backend-app                                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  default                     prom-example-prometheus-node-exporter-7z7dd               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  default                     prometheus-prom-example-kube-promethe-prometheus-0        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         93d
  ingress-nginx               ingress-nginx-controller-7c6974c4d8-cq6qb                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         94d
  kube-system                 coredns-5dd5756b68-bfcgm                                  100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     107d
  kube-system                 etcd-minikube                                             100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         98d
  kube-system                 kube-apiserver-minikube                                   250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         98d
  kube-system                 kube-controller-manager-minikube                          200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         107d
  kube-system                 kube-proxy-r65nv                                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         107d
  kube-system                 kube-scheduler-minikube                                   100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         107d
  kube-system                 storage-provisioner                                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         107d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-4zxkv                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         98d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-msdbf                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         98d
  myapp                       myapp-55c645d9b5-k8g5l                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         92d
  myapp                       myapp-55c645d9b5-rj8cc                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         92d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (8%!)(MISSING)   0 (0%!)(MISSING)
  memory             460Mi (5%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 44m                kube-proxy       
  Normal  Starting                 44m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  44m (x8 over 44m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    44m (x8 over 44m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     44m (x7 over 44m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  44m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           44m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [May24 12:47] systemd[56011]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[May24 13:05] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.

* 
* ==> etcd [dd5097034260] <==
* {"level":"info","ts":"2024-05-24T12:47:42.041929Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":900091,"snapshot-size":"9.3 kB"}
{"level":"info","ts":"2024-05-24T12:47:42.041985Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":17809408,"backend-size":"18 MB","backend-size-in-use-bytes":6197248,"backend-size-in-use":"6.2 MB"}
{"level":"info","ts":"2024-05-24T12:47:42.068782Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":903186}
{"level":"info","ts":"2024-05-24T12:47:42.069288Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-05-24T12:47:42.069403Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 8"}
{"level":"info","ts":"2024-05-24T12:47:42.069413Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 8, commit: 903186, applied: 900091, lastindex: 903186, lastterm: 8]"}
{"level":"info","ts":"2024-05-24T12:47:42.06953Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-24T12:47:42.069947Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-24T12:47:42.069956Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-05-24T12:47:42.070462Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-24T12:47:42.070862Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":739704}
{"level":"info","ts":"2024-05-24T12:47:42.076269Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":740273}
{"level":"info","ts":"2024-05-24T12:47:42.076951Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-24T12:47:42.077559Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-05-24T12:47:42.078608Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-24T12:47:42.078645Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-24T12:47:42.07874Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-05-24T12:47:42.079147Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T12:47:42.079195Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T12:47:42.079201Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T12:47:42.080361Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-24T12:47:42.080445Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-05-24T12:47:42.080479Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-05-24T12:47:42.080518Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-24T12:47:42.080537Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-24T12:47:42.573177Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 8"}
{"level":"info","ts":"2024-05-24T12:47:42.574139Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 8"}
{"level":"info","ts":"2024-05-24T12:47:42.574181Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-05-24T12:47:42.574207Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 9"}
{"level":"info","ts":"2024-05-24T12:47:42.57422Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-05-24T12:47:42.57424Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 9"}
{"level":"info","ts":"2024-05-24T12:47:42.574256Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-05-24T12:47:42.599189Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-24T12:47:42.599358Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-24T12:47:42.599538Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-24T12:47:42.599636Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-24T12:47:42.599675Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-24T12:47:42.60049Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.58.2:2379"}
{"level":"info","ts":"2024-05-24T12:47:42.600525Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-05-24T12:57:42.619382Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":740922}
{"level":"info","ts":"2024-05-24T12:57:42.64512Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":740922,"took":"24.691208ms","hash":2690876757}
{"level":"info","ts":"2024-05-24T12:57:42.645198Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2690876757,"revision":740922,"compact-revision":739704}
{"level":"info","ts":"2024-05-24T13:02:42.620358Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":741209}
{"level":"info","ts":"2024-05-24T13:02:42.640243Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":741209,"took":"19.584334ms","hash":2710779951}
{"level":"info","ts":"2024-05-24T13:02:42.640303Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2710779951,"revision":741209,"compact-revision":740922}
{"level":"info","ts":"2024-05-24T13:07:42.628674Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":741496}
{"level":"info","ts":"2024-05-24T13:07:42.650165Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":741496,"took":"20.699125ms","hash":3482130350}
{"level":"info","ts":"2024-05-24T13:07:42.650276Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3482130350,"revision":741496,"compact-revision":741209}
{"level":"info","ts":"2024-05-24T13:12:42.634969Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":741783}
{"level":"info","ts":"2024-05-24T13:12:42.650075Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":741783,"took":"14.704208ms","hash":1470299095}
{"level":"info","ts":"2024-05-24T13:12:42.650145Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1470299095,"revision":741783,"compact-revision":741496}
{"level":"info","ts":"2024-05-24T13:17:42.621563Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":742070}
{"level":"info","ts":"2024-05-24T13:17:42.643403Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":742070,"took":"21.209833ms","hash":1238662127}
{"level":"info","ts":"2024-05-24T13:17:42.643471Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1238662127,"revision":742070,"compact-revision":741783}
{"level":"info","ts":"2024-05-24T13:22:42.625846Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":742368}
{"level":"info","ts":"2024-05-24T13:22:42.645417Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":742368,"took":"18.613792ms","hash":1847342380}
{"level":"info","ts":"2024-05-24T13:22:42.645528Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1847342380,"revision":742368,"compact-revision":742070}
{"level":"info","ts":"2024-05-24T13:27:42.633326Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":742685}
{"level":"info","ts":"2024-05-24T13:27:42.661321Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":742685,"took":"26.998583ms","hash":494268586}
{"level":"info","ts":"2024-05-24T13:27:42.661417Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":494268586,"revision":742685,"compact-revision":742368}

* 
* ==> etcd [ed41ab707aa8] <==
* {"level":"info","ts":"2024-03-08T08:43:19.653295Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2982189640,"revision":735400,"compact-revision":735113}
{"level":"info","ts":"2024-03-08T08:48:19.657619Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":735688}
{"level":"info","ts":"2024-03-08T08:48:19.661858Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":735688,"took":"3.919125ms","hash":4021737831}
{"level":"info","ts":"2024-03-08T08:48:19.661951Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4021737831,"revision":735688,"compact-revision":735400}
{"level":"info","ts":"2024-03-08T08:53:19.672683Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":735976}
{"level":"info","ts":"2024-03-08T08:53:19.688695Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":735976,"took":"15.352958ms","hash":2805459552}
{"level":"info","ts":"2024-03-08T08:53:19.688777Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2805459552,"revision":735976,"compact-revision":735688}
{"level":"info","ts":"2024-03-08T08:58:19.682792Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":736264}
{"level":"info","ts":"2024-03-08T08:58:19.688635Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":736264,"took":"5.374667ms","hash":570231612}
{"level":"info","ts":"2024-03-08T08:58:19.688712Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":570231612,"revision":736264,"compact-revision":735976}
{"level":"info","ts":"2024-03-08T09:03:19.651323Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":736551}
{"level":"info","ts":"2024-03-08T09:03:19.659718Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":736551,"took":"7.873417ms","hash":1544677001}
{"level":"info","ts":"2024-03-08T09:03:19.659774Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1544677001,"revision":736551,"compact-revision":736264}
{"level":"info","ts":"2024-03-08T09:08:19.661704Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":736837}
{"level":"info","ts":"2024-03-08T09:08:19.669581Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":736837,"took":"7.192251ms","hash":3158977749}
{"level":"info","ts":"2024-03-08T09:08:19.669708Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3158977749,"revision":736837,"compact-revision":736551}
{"level":"info","ts":"2024-03-08T09:13:19.669844Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":737124}
{"level":"info","ts":"2024-03-08T09:13:19.673835Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":737124,"took":"3.632542ms","hash":3472211260}
{"level":"info","ts":"2024-03-08T09:13:19.673906Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3472211260,"revision":737124,"compact-revision":736837}
{"level":"info","ts":"2024-03-08T09:18:19.687842Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":737411}
{"level":"info","ts":"2024-03-08T09:18:19.692134Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":737411,"took":"3.8005ms","hash":2578628426}
{"level":"info","ts":"2024-03-08T09:18:19.692215Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2578628426,"revision":737411,"compact-revision":737124}
{"level":"info","ts":"2024-03-08T09:18:55.695838Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":900091,"local-member-snapshot-index":890090,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-03-08T09:18:55.709708Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":900091}
{"level":"info","ts":"2024-03-08T09:18:55.709871Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":895091}
{"level":"info","ts":"2024-03-08T09:19:23.625859Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000008-00000000000cf8a6.snap"}
{"level":"info","ts":"2024-03-08T09:23:19.712351Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":737698}
{"level":"info","ts":"2024-03-08T09:23:19.715374Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":737698,"took":"2.53975ms","hash":2611709065}
{"level":"info","ts":"2024-03-08T09:23:19.715437Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2611709065,"revision":737698,"compact-revision":737411}
{"level":"info","ts":"2024-03-08T09:28:19.739115Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":737986}
{"level":"info","ts":"2024-03-08T09:28:19.760794Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":737986,"took":"19.404542ms","hash":752434808}
{"level":"info","ts":"2024-03-08T09:28:19.76096Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":752434808,"revision":737986,"compact-revision":737698}
{"level":"info","ts":"2024-03-08T09:33:19.729436Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":738273}
{"level":"info","ts":"2024-03-08T09:33:19.754409Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":738273,"took":"24.400625ms","hash":3152607479}
{"level":"info","ts":"2024-03-08T09:33:19.754464Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3152607479,"revision":738273,"compact-revision":737986}
{"level":"info","ts":"2024-03-08T09:38:19.734648Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":738558}
{"level":"info","ts":"2024-03-08T09:38:19.778159Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":738558,"took":"41.020625ms","hash":129851739}
{"level":"info","ts":"2024-03-08T09:38:19.778279Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":129851739,"revision":738558,"compact-revision":738273}
{"level":"info","ts":"2024-03-08T09:43:19.741124Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":738844}
{"level":"info","ts":"2024-03-08T09:43:19.76173Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":738844,"took":"19.952917ms","hash":3154683164}
{"level":"info","ts":"2024-03-08T09:43:19.761809Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3154683164,"revision":738844,"compact-revision":738558}
{"level":"info","ts":"2024-03-08T09:48:19.747676Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":739131}
{"level":"info","ts":"2024-03-08T09:48:19.757936Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":739131,"took":"9.744958ms","hash":811558794}
{"level":"info","ts":"2024-03-08T09:48:19.757998Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":811558794,"revision":739131,"compact-revision":738844}
{"level":"info","ts":"2024-03-08T09:53:19.775679Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":739417}
{"level":"info","ts":"2024-03-08T09:53:19.782626Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":739417,"took":"6.454167ms","hash":4091810931}
{"level":"info","ts":"2024-03-08T09:53:19.782661Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4091810931,"revision":739417,"compact-revision":739131}
{"level":"info","ts":"2024-03-08T09:58:19.787843Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":739704}
{"level":"info","ts":"2024-03-08T09:58:19.799588Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":739704,"took":"11.06ms","hash":369476388}
{"level":"info","ts":"2024-03-08T09:58:19.799658Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":369476388,"revision":739704,"compact-revision":739417}
{"level":"info","ts":"2024-03-08T10:03:14.885181Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-03-08T10:03:14.889946Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}
{"level":"warn","ts":"2024-03-08T10:03:14.891448Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-03-08T10:03:14.892873Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-03-08T10:03:14.921618Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-03-08T10:03:14.921815Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-03-08T10:03:14.924363Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-03-08T10:03:14.942777Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-03-08T10:03:14.94565Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-03-08T10:03:14.945684Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}

* 
* ==> kernel <==
*  13:32:34 up  3:56,  0 users,  load average: 6.09, 5.53, 5.40
Linux minikube 6.6.22-linuxkit #1 SMP Fri Mar 29 12:21:27 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [72010740c32c] <==
*   "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0308 10:03:14.911556       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0308 10:03:14.911580       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0308 10:03:14.911600       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0308 10:03:14.911618       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0308 10:03:14.911870       1 logging.go:59] [core] [Channel #208 SubChannel #209] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0308 10:03:14.911872       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0308 10:03:14.912736       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0308 10:03:14.917549       1 controller.go:128] Shutting down kubernetes service endpoint reconciler

* 
* ==> kube-apiserver [fc6cc257f5c2] <==
* I0524 13:07:43.575047       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:07:43.575599       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:07:43.575779       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:07:43.577615       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:07:43.577732       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:07:43.577785       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:07:43.582952       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:07:43.583985       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:07:43.586542       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:07:43.586644       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:07:43.587102       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:12:43.573753       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:12:43.578188       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:12:43.582469       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:12:43.582589       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:12:43.582890       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:12:43.585433       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:12:43.585658       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:12:43.587834       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:12:43.588030       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:12:43.589001       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:12:43.589103       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:12:43.589181       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:12:43.589200       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:17:43.557031       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:17:43.557197       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:17:43.561336       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:17:43.561632       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:17:43.561711       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:17:43.561800       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:17:43.561899       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:17:43.562007       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:17:43.562302       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:17:43.562451       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:17:43.564058       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:17:43.564163       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:17:43.564607       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:22:43.550704       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:22:43.559165       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:22:43.559285       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:22:43.564115       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:22:43.564222       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:22:43.564505       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:22:43.564619       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:22:43.564752       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:22:43.564866       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:22:43.564926       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:22:43.564974       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:22:43.565112       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:27:43.553846       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:27:43.555670       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:27:43.556498       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:27:43.556918       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:27:43.557436       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:27:43.557636       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:27:43.557743       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:27:43.558367       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager
I0524 13:27:43.558447       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0524 13:27:43.558538       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1alpha1 to ResourceManager
I0524 13:27:43.558619       1 handler.go:232] Adding GroupVersion monitoring.coreos.com v1 to ResourceManager

* 
* ==> kube-controller-manager [5d13c3fcab63] <==
* I0524 12:48:02.287380       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="38.292µs"
I0524 12:48:02.301878       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-554bd7bf5" duration="44.166µs"
I0524 12:48:03.334461       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-5fb9857f6c" duration="6.65675ms"
I0524 12:48:03.334706       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-5fb9857f6c" duration="123.459µs"
I0524 12:48:06.428024       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="63.583µs"
I0524 12:48:07.090252       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-554bd7bf5" duration="10.730542ms"
I0524 12:48:07.090345       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-554bd7bf5" duration="63.5µs"
I0524 12:48:14.596131       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="48.708µs"
I0524 12:48:16.199839       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-6697c5749b" duration="9.636417ms"
I0524 12:48:16.199978       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-6697c5749b" duration="86.708µs"
I0524 12:48:21.660274       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="36.75µs"
I0524 12:48:30.036725       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="34.917µs"
I0524 12:48:35.185033       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="51.833µs"
I0524 12:48:44.596423       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="60.959µs"
I0524 12:48:45.611575       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="75.334µs"
I0524 12:49:11.182110       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="228.958µs"
I0524 12:49:26.586550       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="50.75µs"
I0524 12:49:26.596454       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="30.791µs"
I0524 12:49:37.607832       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="42.125µs"
I0524 12:50:42.775873       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="264.791µs"
I0524 12:50:52.082712       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="91.542µs"
I0524 12:50:57.603441       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="60µs"
I0524 12:51:04.593968       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="86.667µs"
I0524 12:53:29.345304       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="65.417µs"
I0524 12:53:42.595929       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="85.292µs"
I0524 12:53:43.707839       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="80.375µs"
I0524 12:53:56.597543       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="80.666µs"
I0524 12:58:41.366073       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="325.709µs"
I0524 12:58:51.746085       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="100.5µs"
I0524 12:58:53.608714       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="90.75µs"
I0524 12:59:04.592910       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="106.333µs"
I0524 13:03:47.602707       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="185.834µs"
I0524 13:03:55.848577       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="77.917µs"
I0524 13:04:01.611437       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="102.75µs"
I0524 13:04:09.602265       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="127.167µs"
I0524 13:09:02.506843       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="123.042µs"
I0524 13:09:05.598036       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="66.166µs"
I0524 13:09:15.603375       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="85.334µs"
I0524 13:09:17.588916       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="61.834µs"
I0524 13:14:13.407164       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="95.166µs"
I0524 13:14:16.513792       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="65.625µs"
I0524 13:14:25.580896       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="83.375µs"
I0524 13:14:27.580905       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="101.875µs"
I0524 13:19:26.504225       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="116.417µs"
I0524 13:19:27.534570       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="57.083µs"
I0524 13:19:41.563408       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="56.583µs"
I0524 13:19:41.567541       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="36.042µs"
I0524 13:22:45.038623       1 event.go:307] "Event occurred" object="default/alertmanager-prom-example-kube-promethe-alertmanager" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod alertmanager-prom-example-kube-promethe-alertmanager-0 in StatefulSet alertmanager-prom-example-kube-promethe-alertmanager successful"
I0524 13:23:28.652473       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/flask-deployment-7bdf46c4bc" duration="5.25µs"
I0524 13:23:28.652748       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/flask-deployment-79d9cb5779" duration="14.833µs"
I0524 13:23:36.820541       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-deployment-778c8bf6d5" duration="7.792µs"
I0524 13:23:36.821607       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-deployment-b8f4779b7" duration="5.666µs"
I0524 13:24:29.844986       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="149.334µs"
I0524 13:24:36.042984       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="85.5µs"
I0524 13:24:42.560274       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="79.708µs"
I0524 13:24:47.572336       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="61.875µs"
I0524 13:29:36.927015       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="555.625µs"
I0524 13:29:37.943541       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="65.5µs"
I0524 13:29:49.570694       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="181.542µs"
I0524 13:29:52.564032       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="349.875µs"

* 
* ==> kube-controller-manager [eb824310f0d7] <==
* I0308 08:57:33.887616       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-deployment-b8f4779b7" duration="24.541µs"
I0308 08:57:33.887699       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="129.292µs"
I0308 08:57:33.887719       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/flask-deployment-7bdf46c4bc" duration="28.834µs"
I0308 08:57:33.887749       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-applicationset-controller-64c77cff5d" duration="42.916µs"
I0308 08:57:33.887808       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-notifications-controller-679d59bf4b" duration="50.459µs"
I0308 08:57:33.887858       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-deployment-778c8bf6d5" duration="24.125µs"
I0308 08:57:33.887865       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-554bd7bf5" duration="72.375µs"
I0308 08:57:33.887891       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-5fb9857f6c" duration="45.875µs"
I0308 08:57:33.887929       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/flask-deployment-79d9cb5779" duration="30.75µs"
I0308 08:57:35.380511       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="34.416µs"
I0308 08:57:36.831918       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="58.041µs"
I0308 08:57:50.830302       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="73µs"
I0308 09:02:25.278557       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="271.667µs"
I0308 09:02:38.687376       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="59µs"
I0308 09:02:39.779669       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="80.708µs"
I0308 09:02:49.799607       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="237.916µs"
I0308 09:07:37.417215       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="74µs"
I0308 09:07:40.519843       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="55.583µs"
I0308 09:07:51.792211       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="135.417µs"
I0308 09:07:53.794597       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="297.125µs"
I0308 09:12:38.889728       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="52.125µs"
I0308 09:12:44.026826       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="49.458µs"
I0308 09:12:51.792137       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="134µs"
I0308 09:12:56.786364       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="95.25µs"
I0308 09:17:48.359016       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="420.542µs"
I0308 09:17:52.515373       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="104.375µs"
I0308 09:18:00.781980       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="48.167µs"
I0308 09:18:04.792299       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="85.292µs"
I0308 09:22:50.912592       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="58.25µs"
I0308 09:22:57.099322       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="60.083µs"
I0308 09:23:04.775002       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="62.583µs"
I0308 09:23:08.800407       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="125.125µs"
I0308 09:28:00.527219       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="160.125µs"
I0308 09:28:01.577538       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="69.291µs"
I0308 09:28:12.786456       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="78.75µs"
I0308 09:28:12.791680       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="88.667µs"
I0308 09:33:07.102590       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="200.333µs"
I0308 09:33:12.215651       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="136.541µs"
I0308 09:33:18.748116       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="91.25µs"
I0308 09:33:26.756679       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="282.083µs"
I0308 09:38:18.813009       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="206.834µs"
I0308 09:38:20.876107       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="123.833µs"
I0308 09:38:31.741905       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="305.375µs"
I0308 09:38:33.730930       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="155.333µs"
I0308 09:43:25.961274       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="351.667µs"
I0308 09:43:26.980533       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="96.917µs"
I0308 09:43:30.042202       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="89.5µs"
I0308 09:43:42.731879       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="215.666µs"
I0308 09:48:30.994132       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="224.125µs"
I0308 09:48:37.126827       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="143.625µs"
I0308 09:48:43.745133       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="133.625µs"
I0308 09:48:49.747002       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="57.917µs"
I0308 09:53:42.284090       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="479.584µs"
I0308 09:53:48.423589       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="168.583µs"
I0308 09:53:57.737422       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="93.458µs"
I0308 09:53:59.743405       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="60.333µs"
I0308 09:58:47.793542       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="223.083µs"
I0308 09:58:56.984435       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="58.708µs"
I0308 09:59:02.738925       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="52.25µs"
I0308 09:59:10.752913       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="myapp/myapp-55c645d9b5" duration="136.417µs"

* 
* ==> kube-proxy [a20e92ec140c] <==
* I0524 12:47:48.563053       1 server_others.go:69] "Using iptables proxy"
I0524 12:47:48.670620       1 node.go:141] Successfully retrieved node IP: 192.168.58.2
I0524 12:47:48.871842       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0524 12:47:48.873239       1 server_others.go:152] "Using iptables Proxier"
I0524 12:47:48.873272       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0524 12:47:48.873278       1 server_others.go:438] "Defaulting to no-op detect-local"
I0524 12:47:48.873647       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0524 12:47:48.874534       1 server.go:846] "Version info" version="v1.28.3"
I0524 12:47:48.874552       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0524 12:47:48.877636       1 config.go:188] "Starting service config controller"
I0524 12:47:48.877682       1 shared_informer.go:311] Waiting for caches to sync for service config
I0524 12:47:48.877682       1 config.go:315] "Starting node config controller"
I0524 12:47:48.878118       1 shared_informer.go:311] Waiting for caches to sync for node config
I0524 12:47:48.949971       1 config.go:97] "Starting endpoint slice config controller"
I0524 12:47:48.950002       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0524 12:47:48.979557       1 shared_informer.go:318] Caches are synced for service config
I0524 12:47:49.050581       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0524 12:47:49.050582       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [ff14883828c6] <==
* I0303 14:52:30.586843       1 server_others.go:69] "Using iptables proxy"
I0303 14:52:30.684386       1 node.go:141] Successfully retrieved node IP: 192.168.58.2
I0303 14:52:30.722115       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0303 14:52:30.784082       1 server_others.go:152] "Using iptables Proxier"
I0303 14:52:30.784118       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0303 14:52:30.784124       1 server_others.go:438] "Defaulting to no-op detect-local"
I0303 14:52:30.784136       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0303 14:52:30.785218       1 server.go:846] "Version info" version="v1.28.3"
I0303 14:52:30.785276       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0303 14:52:30.787265       1 config.go:188] "Starting service config controller"
I0303 14:52:30.787279       1 config.go:315] "Starting node config controller"
I0303 14:52:30.787345       1 config.go:97] "Starting endpoint slice config controller"
I0303 14:52:30.789043       1 shared_informer.go:311] Waiting for caches to sync for service config
I0303 14:52:30.789008       1 shared_informer.go:311] Waiting for caches to sync for node config
I0303 14:52:30.789140       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0303 14:52:30.889820       1 shared_informer.go:318] Caches are synced for node config
I0303 14:52:30.889825       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0303 14:52:30.889837       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [7c3b192a134e] <==
* I0524 12:47:41.549665       1 serving.go:348] Generated self-signed cert in-memory
I0524 12:47:43.557550       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0524 12:47:43.557571       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0524 12:47:43.563107       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0524 12:47:43.563773       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0524 12:47:43.563858       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0524 12:47:43.564271       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0524 12:47:43.564370       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0524 12:47:43.564384       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0524 12:47:43.564402       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0524 12:47:43.564409       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0524 12:47:43.665627       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0524 12:47:43.665649       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0524 12:47:43.665664       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [ae6b3be8164a] <==
* I0303 14:52:26.096773       1 serving.go:348] Generated self-signed cert in-memory
W0303 14:52:27.819522       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0303 14:52:27.819961       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0303 14:52:27.819974       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0303 14:52:27.819978       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0303 14:52:27.884478       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0303 14:52:27.884528       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0303 14:52:27.887417       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0303 14:52:27.887476       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0303 14:52:27.888764       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0303 14:52:27.889556       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0303 14:52:27.989909       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0308 10:03:14.962229       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0308 10:03:14.964456       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0308 10:03:14.964683       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0308 10:03:14.968646       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* May 24 13:30:03 minikube kubelet[1587]: I0524 13:30:03.550810    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:30:03 minikube kubelet[1587]: E0524 13:30:03.551124    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:30:03 minikube kubelet[1587]: I0524 13:30:03.552881    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:30:03 minikube kubelet[1587]: E0524 13:30:03.553042    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:30:04 minikube kubelet[1587]: E0524 13:30:04.553056    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:30:16 minikube kubelet[1587]: I0524 13:30:16.556942    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:30:16 minikube kubelet[1587]: E0524 13:30:16.557133    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:30:17 minikube kubelet[1587]: I0524 13:30:17.548819    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:30:17 minikube kubelet[1587]: E0524 13:30:17.549252    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:30:18 minikube kubelet[1587]: E0524 13:30:18.552673    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:30:31 minikube kubelet[1587]: I0524 13:30:31.548375    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:30:31 minikube kubelet[1587]: E0524 13:30:31.548650    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:30:32 minikube kubelet[1587]: I0524 13:30:32.545658    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:30:32 minikube kubelet[1587]: E0524 13:30:32.545926    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:30:33 minikube kubelet[1587]: E0524 13:30:33.552813    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:30:44 minikube kubelet[1587]: I0524 13:30:44.545655    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:30:44 minikube kubelet[1587]: E0524 13:30:44.546112    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:30:45 minikube kubelet[1587]: E0524 13:30:45.559524    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:30:46 minikube kubelet[1587]: I0524 13:30:46.546865    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:30:46 minikube kubelet[1587]: E0524 13:30:46.547279    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:30:55 minikube kubelet[1587]: I0524 13:30:55.545159    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:30:55 minikube kubelet[1587]: E0524 13:30:55.545385    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:30:58 minikube kubelet[1587]: E0524 13:30:58.553149    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:31:00 minikube kubelet[1587]: I0524 13:31:00.549075    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:31:00 minikube kubelet[1587]: E0524 13:31:00.549239    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:31:09 minikube kubelet[1587]: I0524 13:31:09.609574    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:31:09 minikube kubelet[1587]: E0524 13:31:09.610587    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:31:09 minikube kubelet[1587]: E0524 13:31:09.616995    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:31:11 minikube kubelet[1587]: I0524 13:31:11.547643    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:31:11 minikube kubelet[1587]: E0524 13:31:11.547940    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:31:22 minikube kubelet[1587]: E0524 13:31:22.548845    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:31:24 minikube kubelet[1587]: I0524 13:31:24.546999    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:31:24 minikube kubelet[1587]: E0524 13:31:24.547549    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:31:25 minikube kubelet[1587]: I0524 13:31:25.546219    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:31:25 minikube kubelet[1587]: E0524 13:31:25.546371    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:31:36 minikube kubelet[1587]: I0524 13:31:36.545353    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:31:36 minikube kubelet[1587]: E0524 13:31:36.547670    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:31:37 minikube kubelet[1587]: E0524 13:31:37.558828    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:31:39 minikube kubelet[1587]: I0524 13:31:39.546153    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:31:39 minikube kubelet[1587]: E0524 13:31:39.546423    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:31:48 minikube kubelet[1587]: I0524 13:31:48.547721    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:31:48 minikube kubelet[1587]: E0524 13:31:48.548396    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:31:52 minikube kubelet[1587]: I0524 13:31:52.549270    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:31:52 minikube kubelet[1587]: E0524 13:31:52.549583    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:31:52 minikube kubelet[1587]: E0524 13:31:52.551882    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:32:00 minikube kubelet[1587]: I0524 13:32:00.544435    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:32:00 minikube kubelet[1587]: E0524 13:32:00.544762    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:32:04 minikube kubelet[1587]: E0524 13:32:04.546300    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:32:07 minikube kubelet[1587]: I0524 13:32:07.544991    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:32:07 minikube kubelet[1587]: E0524 13:32:07.545198    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:32:14 minikube kubelet[1587]: I0524 13:32:14.545201    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:32:14 minikube kubelet[1587]: E0524 13:32:14.545741    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:32:15 minikube kubelet[1587]: E0524 13:32:15.560185    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:32:21 minikube kubelet[1587]: I0524 13:32:21.545911    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:32:21 minikube kubelet[1587]: E0524 13:32:21.546093    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"
May 24 13:32:26 minikube kubelet[1587]: E0524 13:32:26.546315    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-app\" with ImagePullBackOff: \"Back-off pulling image \\\"backend_app:1.0\\\"\"" pod="default/backend-app" podUID="bca3811d-b4d1-4a14-85d1-d56d7a4643d8"
May 24 13:32:29 minikube kubelet[1587]: I0524 13:32:29.545875    1587 scope.go:117] "RemoveContainer" containerID="a3e49ab869017fb08a9dfb2575a36305752fe6b16c686fb90c41c72ea4895806"
May 24 13:32:29 minikube kubelet[1587]: E0524 13:32:29.546062    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-k8g5l_myapp(18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab)\"" pod="myapp/myapp-55c645d9b5-k8g5l" podUID="18c4c727-b2ed-4a7d-b17c-a6cec5ab59ab"
May 24 13:32:32 minikube kubelet[1587]: I0524 13:32:32.542923    1587 scope.go:117] "RemoveContainer" containerID="c56a0a054a1c936047d53a4bdf3c2d4a1f496aeaee003769959aa1701978fc2d"
May 24 13:32:32 minikube kubelet[1587]: E0524 13:32:32.543113    1587 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=myapp pod=myapp-55c645d9b5-rj8cc_myapp(1cb0ff31-860a-4c6f-80f0-bffa4033f717)\"" pod="myapp/myapp-55c645d9b5-rj8cc" podUID="1cb0ff31-860a-4c6f-80f0-bffa4033f717"

* 
* ==> kubernetes-dashboard [4c5a62deb665] <==
* 2024/05/24 12:47:48 Using namespace: kubernetes-dashboard
2024/05/24 12:47:48 Using in-cluster config to connect to apiserver
2024/05/24 12:47:48 Using secret token for csrf signing
2024/05/24 12:47:48 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/05/24 12:47:48 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": x509: certificate signed by unknown authority

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0x400069fa78)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x2c0
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0x4000622080)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x7c
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x14957f0?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x30
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1bc

* 
* ==> kubernetes-dashboard [e63f08286d22] <==
* 2024/05/24 12:48:00 Using namespace: kubernetes-dashboard
2024/05/24 12:48:00 Using in-cluster config to connect to apiserver
2024/05/24 12:48:00 Using secret token for csrf signing
2024/05/24 12:48:00 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/05/24 12:48:00 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/05/24 12:48:00 Successful initial request to the apiserver, version: v1.28.3
2024/05/24 12:48:00 Generating JWE encryption key
2024/05/24 12:48:00 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/05/24 12:48:00 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/05/24 12:48:01 Initializing JWE encryption key from synchronized object
2024/05/24 12:48:01 Creating in-cluster Sidecar client
2024/05/24 12:48:01 Serving insecurely on HTTP port: 9090
2024/05/24 12:48:01 Successful request to sidecar
2024/05/24 12:48:00 Starting overwatch

* 
* ==> storage-provisioner [229e259de056] <==
* I0524 12:47:47.365998       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0524 12:47:47.573432       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

* 
* ==> storage-provisioner [3b9e29c29d5e] <==
* I0524 12:48:04.705222       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0524 12:48:04.717978       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0524 12:48:04.718529       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0524 12:48:22.157976       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0524 12:48:22.158102       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_da146cc8-72e5-4fee-8aed-2cbd184b5db7!
I0524 12:48:22.158644       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a8dc6b00-e150-453d-adc1-488e47364b36", APIVersion:"v1", ResourceVersion:"740639", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_da146cc8-72e5-4fee-8aed-2cbd184b5db7 became leader
I0524 12:48:22.259320       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_da146cc8-72e5-4fee-8aed-2cbd184b5db7!

